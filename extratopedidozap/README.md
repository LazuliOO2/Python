# ğŸ§  Extrator de Pedidos com LLM e Pydantic

## ğŸ“¦ DescriÃ§Ã£o

Este projeto tem como objetivo **extrair automaticamente pedidos em formato estruturado (JSON)** a partir de **mensagens de texto nÃ£o estruturadas** enviadas por clientes.  
Ele utiliza **modelos de linguagem (LLM)** como o [Ollama](https://ollama.com) com o modelo **Gemma 3:1B**, combinados com **validaÃ§Ã£o de dados via Pydantic**, para garantir que as informaÃ§Ãµes extraÃ­das estejam corretas e no formato esperado.

O sistema realiza as seguintes etapas:
- ğŸ§  InterpretaÃ§Ã£o da mensagem original do cliente.  
- ğŸ” ExtraÃ§Ã£o dos campos relevantes (cliente, endereÃ§o, itens, pagamento etc.).  
- ğŸ§¹ NormalizaÃ§Ã£o e validaÃ§Ã£o dos dados com base em um esquema definido.  
- ğŸ’¾ Salvamento dos pedidos em arquivos JSON e JSONL.  

---

## ğŸ“ Estrutura do Projeto

```
ğŸ“‚ CONSULTORIADESAFIO
â”œâ”€â”€ ğŸ“ env/                  â†’ Ambiente virtual (opcional)
â”œâ”€â”€ ğŸ“ out/                  â†’ SaÃ­da dos pedidos extraÃ­dos (JSON e JSONL)
â”œâ”€â”€ ğŸ“„ app.py               â†’ Script principal para extraÃ§Ã£o, validaÃ§Ã£o e salvamento (com LLM)
â”œâ”€â”€ ğŸ“„ pedido_extractor_no_llm.py â†’ VersÃ£o B sem LLM (regex + heurÃ­sticas)
â””â”€â”€ ğŸ“„ requirements.txt     â†’ DependÃªncias do projeto
```

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python**: Linguagem principal do projeto.  
- **Pydantic**: Para definiÃ§Ã£o de esquemas e validaÃ§Ã£o dos dados extraÃ­dos.  
- **Requests**: Para realizar requisiÃ§Ãµes ao servidor do modelo LLM.  
- **Ollama**: Backend de inferÃªncia LLM local.  
- **JSON**: Formato estruturado de saÃ­da dos pedidos.  
- **Regex (re)**: Para limpeza e sanitizaÃ§Ã£o de respostas do modelo.

---

## âš™ï¸ InstalaÃ§Ã£o

1. **Clonar o repositÃ³rio:**

```sh
git clone <URL_DO_REPOSITORIO>
cd CONSULTORIADESAFIO
```

2. **Criar e ativar o ambiente virtual (opcional):**

```sh
python -m venv env
source env/bin/activate     # ou env\Scripts\activate no Windows
```

3. **Instalar as dependÃªncias:**

```sh
pip install -r requirements.txt
```

4. **Instalar e rodar o Ollama (caso ainda nÃ£o tenha):**

- Acesse [https://ollama.com/download](https://ollama.com/download)  
- Instale o Ollama no seu sistema e baixe o modelo utilizado:

```sh
ollama pull gemma3:1b
```

---

## ğŸš€ ExecuÃ§Ã£o

Para rodar o projeto com LLM, basta executar o arquivo `app.py`:

```sh
python app.py
```

âœ… Exemplo de mensagem processada:

```
"Oi! Aqui Ã© o JoÃ£o. Quero 2 pizzas grandes de calabresa (sem cebola) e uma coca 2L. 
Entregar na Rua das Flores, 55. Pago em dinheiro. Por favor, deixar na portaria."
```

âœ… SaÃ­da esperada (JSON estruturado):

```json
{
  "cliente": "JoÃ£o",
  "telefone": null,
  "endereco": "Rua das Flores, 55",
  "pagamento": "dinheiro",
  "itens": [
    {
      "produto": "pizza de calabresa",
      "tamanho": "grande",
      "quantidade": 2,
      "observacoes": ["sem cebola"]
    },
    {
      "produto": "coca-cola",
      "tamanho": "2L",
      "quantidade": 1,
      "observacoes": null
    }
  ],
  "observacoes_gerais": ["deixar na portaria"],
  "mensagem_original": "Oi! Aqui Ã© o JoÃ£o..."
}
```

ApÃ³s a execuÃ§Ã£o, os resultados sÃ£o automaticamente salvos em:

- ğŸ“„ `out/pedido-YYYY-MM-DDTHH-MM-SS.json` â€“ Pedido individual.
- ğŸ“„ `out/pedidos.jsonl` â€“ HistÃ³rico com mÃºltiplos pedidos (um por linha).

---

## ğŸ“š Funcionalidades Principais

- âœ… **ExtraÃ§Ã£o robusta:** Interpreta pedidos em linguagem natural com precisÃ£o.  
- âœ… **SanitizaÃ§Ã£o automÃ¡tica:** Remove ruÃ­dos e corrige respostas imperfeitas do LLM.  
- âœ… **ValidaÃ§Ã£o com Pydantic:** Garante integridade dos dados e tipos corretos.  
- âœ… **Fallback inteligente:** Se o modelo esquecer campos obrigatÃ³rios, o sistema preenche automaticamente.  
- âœ… **ExportaÃ§Ã£o versÃ¡til:** Permite salvar cada pedido individualmente ou em lote (JSONL).

---

## ğŸ§ª Sem Ollama? Use a VersÃ£o B!

Caso vocÃª **nÃ£o queira ou nÃ£o possa instalar o Ollama**, existe uma **VersÃ£o B** deste projeto (`pedido_extractor_no_llm.py`) que funciona **100% localmente**, sem depender de modelos de linguagem.  

Essa versÃ£o utiliza **regex, heurÃ­sticas e Pydantic** para identificar os mesmos campos (cliente, endereÃ§o, itens, pagamento, observaÃ§Ãµes etc.) diretamente a partir do texto, com alta precisÃ£o em cenÃ¡rios comuns.

### ğŸ§° Como usar a versÃ£o B

1. Certifique-se de ter o Python instalado.  
2. Instale a dependÃªncia mÃ­nima:
   ```bash
   pip install pydantic
   ```
3. Execute o script diretamente:
   ```bash
   python pedido_extractor_no_llm.py
   ```

âœ… Ele farÃ¡ a extraÃ§Ã£o dos pedidos da mesma forma e tambÃ©m salvarÃ¡ os resultados em:

- ğŸ“„ `out/pedido-YYYY-MM-DDTHH-MM-SS.json` â€“ pedido individual.  
- ğŸ“„ `out/pedidos.jsonl` â€“ histÃ³rico em formato JSONL.

---

### ğŸ§  Gemma 1B â€“ Requisitos de Hardware (estimados)

| Ambiente | MÃ­nimo recomendÃ¡vel | Ideal para rodar bem |
|----------|---------------------|----------------------|
| ğŸ’» **CPU (sem GPU)** | 8 GB RAM total / ~4 GB RAM livre <br> CPU 4 nÃºcleos (x86_64) | 16 GB RAM total / 8 GB livre <br> CPU 6+ nÃºcleos |
| âš™ï¸ **GPU (recomendado)** | GPU com **2 GB VRAM** (quantizado em 4-bit) | GPU com **4 GB+ VRAM** para rodar fluido e mais rÃ¡pido |
| ğŸ’¾ **Armazenamento** | ~1,2 GB (modelo em 4-bit quantizado) | ~3 GB se usar versÃµes em 8-bit / FP16 |
| ğŸ“¦ **Rede / Download** | ~1 GB de download do modelo (gemma:1b) | â€” |

---

### ğŸ“Š Dica prÃ¡tica (testada com Ollama)

| ConfiguraÃ§Ã£o | Tempo mÃ©dio de resposta |
|--------------|--------------------------|
| ğŸ’» i7 + 16 GB RAM (sem GPU) | ~2.5s â€“ 6s por resposta curta |
| âš™ï¸ Ryzen 5 + 32 GB RAM + GTX 1650 (4 GB) | ~0.4s â€“ 1.2s por resposta |
| ğŸ Apple M1 (16 GB) | ~1s â€“ 2s por resposta |
| â˜ï¸ VPS com 2 vCPU e 4 GB RAM | âŒ Pode travar / muito lento |

ğŸ’¡ **Dica:** use a versÃ£o com LLM quando precisar lidar com **mensagens complexas ou pouco estruturadas**.  
Se o objetivo for apenas extrair pedidos simples e comuns (pizzas, lanches, bebidas etc.), a **VersÃ£o B** jÃ¡ serÃ¡ mais que suficiente.

---

### ğŸ” DiferenÃ§as principais

| Recurso | VersÃ£o com LLM (`app.py`) | VersÃ£o B (`pedido_extractor_no_llm.py`) |
|--------|----------------------------|----------------------------------------|
| ExtraÃ§Ã£o semÃ¢ntica (interpretaÃ§Ã£o complexa) | âœ… Alta (via modelo LLM) | âš ï¸ Limitada a padrÃµes comuns |
| InstalaÃ§Ã£o de modelo | âœ… NecessÃ¡ria (Ollama + modelo) | âŒ NÃ£o necessÃ¡ria |
| PrecisÃ£o em frases ambÃ­guas | âœ… Melhor | âš ï¸ Depende do texto |
| ExecuÃ§Ã£o local e offline | âœ… Sim | âœ… Sim |
| DependÃªncias | Python + Pydantic + Requests + Ollama | Apenas Python + Pydantic |

---

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas! Caso queira melhorar a extraÃ§Ã£o, adicionar novos campos ou integrar outros modelos, sinta-se Ã  vontade para abrir uma **Pull Request**.

---

## ğŸ“œ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a **MIT** â€“ veja o arquivo LICENSE para mais detalhes.
