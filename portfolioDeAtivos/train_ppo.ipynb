{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ff6edd",
   "metadata": {},
   "source": [
    "### Semana 1\n",
    "A2C puro para debug e validação do pipeline.\n",
    "\n",
    "### Semana 2\n",
    "PPO sem GAE para estabilizar updates.\n",
    "\n",
    "> **Essa é a Semana 2**\n",
    "\n",
    "PPO significa *Proximal Policy Optimization*, um método de treinamento usado em aprendizado por reforço. A ideia é treinar um agente, tipo um robô virtual ou um modelo de linguagem, para tomar decisões que rendem melhores resultados ao longo do tempo. Ele faz isso ajustando a política do agente, mas sem deixar as mudanças malucas demais, porque modelos adoram pirar quando você deixa solto.\n",
    "\n",
    "PPO controla mudanças descontroladas usando duas estratégias:\n",
    "\n",
    "#### 1. Clipping da razão das políticas\n",
    "- Ele compara a nova política com a antiga, transformando isso em uma razão: `prob_nova / prob_antiga`.\n",
    "- Se essa razão tenta crescer demais, o algoritmo corta.\n",
    "- Isso impede que o modelo dê saltos gigantes que quase sempre acabam em desastre.\n",
    "\n",
    "#### 2. Função de perda que pune exageros\n",
    "- Além do clipping, há uma função de perda que desincentiva atualizações agressivas.\n",
    "- Mesmo que o modelo tente inovar de um jeito duvidoso, ele recebe uma punição bem didática.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247c66a9",
   "metadata": {},
   "source": [
    "O que realmente acontece com batches pequenos\n",
    "\n",
    "Quando você atualiza a rede com todos os 256 dados de uma vez, você calcula um gradiente muito estável, super suave, super “matematicamente bonitinho”.\n",
    "\n",
    "Mas:\n",
    "\n",
    "fica pesado computacionalmente\n",
    "\n",
    "a rede dá poucos updates por rollout (porque cada update é custoso)\n",
    "\n",
    "Quando você usa minibatches, tipo 64:\n",
    "\n",
    "você divide em 4 atualizações menores\n",
    "\n",
    "cada update é mais leve\n",
    "\n",
    "mas cada um tem um pouco mais de ruído (porque só vê parte dos dados)\n",
    "\n",
    "Esse ruído, por incrível que pareça, não é ruim.\n",
    "Ele ajuda a escapar de mínimos ruins e evita que a rede fique certinha demais em cima do rollout atual.\n",
    "\n",
    "É o mesmo motivo de SGD ser usado no mundo inteiro em vez de “usar todos os dados de uma vez”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a2bbbfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo encontrado! Carregando dados locais...\n",
      "            BOVA11.SA  SMAL11.SA  IVVB11.SA  PETR4.SA   VALE3.SA  ITUB4.SA  \\\n",
      "Date                                                                         \n",
      "2015-01-01        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-02  47.259998  52.020000  55.799999  2.572509  10.505502  9.372999   \n",
      "2015-01-03        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-04        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-05  46.320000  50.549999  55.750000  2.352637  10.347521  9.420101   \n",
      "\n",
      "                   SPY        QQQ         IWM        EEM         GLD  \\\n",
      "Date                                                                   \n",
      "2015-01-01         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-02  171.093704  94.906532  103.315140  30.789457  114.080002   \n",
      "2015-01-03         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-04         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-05  168.003784  93.514397  101.933861  30.241489  115.800003   \n",
      "\n",
      "                  TLT     BTC-USD  ETH-USD  BNB-USD  SOL-USD  ADA-USD  \n",
      "Date                                                                   \n",
      "2015-01-01        NaN  314.248993      NaN      NaN      NaN      NaN  \n",
      "2015-01-02  95.468956  315.032013      NaN      NaN      NaN      NaN  \n",
      "2015-01-03        NaN  281.082001      NaN      NaN      NaN      NaN  \n",
      "2015-01-04        NaN  264.195007      NaN      NaN      NaN      NaN  \n",
      "2015-01-05  96.968613  274.473999      NaN      NaN      NaN      NaN  \n",
      "(3977, 17)\n",
      "NUM_ATIVOS = 17\n",
      "-0.019455103321589475\n",
      "Soma: 0.9999999999999999\n",
      "Min: 0.05767012687427912\n",
      "Max: 0.07727797001153403\n",
      "[0.07727797 0.05767013 0.05767013 0.05767013 0.05767013]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olazu\\OneDrive\\Documentos\\Python\\portfolioDeAtivos\\portfolioDeAtivos\\teste.py:154: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  retornos = precos.pct_change().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: torch.Size([1, 37])\n",
      "action_probs shape: torch.Size([1, 34])\n",
      "sum probs: 1.0000001192092896\n",
      "Ep 0 | reward -0.00004 | regime neutro\n",
      "Ep 200 | reward -0.01729 | regime neutro\n",
      "Ep 400 | reward -0.10698 | regime alta_vol\n",
      "Ep 600 | reward 0.00055 | regime bull\n",
      "Ep 800 | reward -0.00024 | regime bull\n",
      "Ep 1000 | reward 0.00074 | regime bull\n",
      "Ep 1200 | reward -0.05478 | regime alta_vol\n",
      "Ep 1400 | reward -0.16539 | regime alta_vol\n",
      "Ep 1600 | reward -0.01537 | regime alta_vol\n",
      "Ep 1800 | reward -0.04339 | regime neutro\n",
      "Ep 2000 | reward -0.00163 | regime alta_vol\n",
      "Ep 2200 | reward 0.01858 | regime bull\n",
      "Ep 2400 | reward -0.01316 | regime alta_vol\n",
      "Ep 2600 | reward -0.09552 | regime alta_vol\n",
      "Ep 2800 | reward -0.44242 | regime bear\n",
      "Ep 3000 | reward -0.39315 | regime bear\n",
      "Ep 3200 | reward -0.30454 | regime bear\n",
      "Ep 3400 | reward -0.01185 | regime alta_vol\n",
      "Ep 3600 | reward 0.01152 | regime bull\n",
      "Ep 3800 | reward -0.00657 | regime alta_vol\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "from rl_env import PortfolioEnv, PolicyMLP, ValueMLP\n",
    "from app import (\n",
    "    ret, r_media, v_media, dd,\n",
    "    regime_ids, cluster_ids, NUM_ATIVOS,\n",
    "    aplicar_acao_portfolio\n",
    ")\n",
    "\n",
    "\n",
    "#? Hiperparâmetros\n",
    "\n",
    "AÇÕES_POR_ATIVOS = 2\n",
    "num_actions = NUM_ATIVOS * AÇÕES_POR_ATIVOS\n",
    "\n",
    "state_dim = NUM_ATIVOS + 3 + NUM_ATIVOS # pesos + (r_media, v_media, dd) + clusters\n",
    "\n",
    "#A política é só a função que diz:dado um estado → qual probabilidade dou para cada ação\n",
    "\n",
    "# quão neurótico o PPO vai ser durante o treino.\n",
    "gamma = 0.99 # Diz o quanto o agente valoriza recompensas futuras.\n",
    "clip_eps = 0.2 # É o “clipe” do PPO, a margem de tolerância de mudança da política.Com 0.2, você deixa a política mudar no máximo ~20% por atualização (em termos de probabilidade relativa)\n",
    "ppo_epochs = 4           # quantas vezes reaproveitar o mesmo rollout\n",
    "rollout_len = 256        # passos por iteração de PPO\n",
    "batch_size = 64 # Você pega esses 256 dados e treina a rede em 4 blocos de 64 cada vez\n",
    "lr = 1e-4 # Taxa de aprendizado do Adam.Quando a rede neural vê um erro (loss), ela tenta ajustar os pesos pra diminuir esse erro.\n",
    "entropy_coef = 0.01 # Entropia alta significa que a política está mais “aleatória”, explorando mais.Se for 0, o agente tende a ficar determinístico muito rápido e pode travar em política ruim.\n",
    "value_coef = 0.5 # Peso da loss da função de valor na loss total. loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "#? Modelos\n",
    "policy = PolicyMLP(state_dim, num_actions)\n",
    "value = ValueMLP(state_dim)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "policy.apply(init_weights)\n",
    "value.apply(init_weights)\n",
    "\n",
    "#value.parameters() = “tudo que deve ser treinado dentro da rede value” e outro parametro é otimizado adam \n",
    "optimizerP = optim.Adam(policy.parameters(), lr=lr)\n",
    "optimizerV = optim.Adam(value.parameters(), lr=lr)\n",
    "\n",
    "#? Ambiente\n",
    "env = PortfolioEnv(ret, r_media, v_media, dd, regime_ids, cluster_ids)\n",
    "\n",
    "#? Função auxiliares \n",
    "def compute_returns_and_advantages(rewards, dones, values, gamma=0.99):\n",
    "    \"\"\"\n",
    "    PPO sem GAE:\n",
    "    - returns = soma de recompensas descontadas (MC)\n",
    "    - advantages = returns - values\n",
    "    \"\"\"\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.float32)\n",
    "    values = np.array(values, dtype=np.float32)\n",
    "\n",
    "    T = len(rewards)\n",
    "    returns = np.zeros(T, dtype=np.float32)\n",
    "    running_return = 0.0\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        if dones[t] == 1.0:\n",
    "            running_return = 0.0\n",
    "        running_return = rewards[t] + gamma * running_return\n",
    "        returns[t] = running_return\n",
    "\n",
    "    advantages = returns - values\n",
    "    # normaliza advantage pra não ficar tudo minúsculo\n",
    "    adv_mean = advantages.mean()\n",
    "    adv_std = advantages.std() + 1e-8\n",
    "    advantages = (advantages - adv_mean) / adv_std\n",
    "\n",
    "    return returns, advantages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3f81104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO] Iter 0 | rollout_reward=0.046033 | avg_reward=0.000180 | avg_return=0.006202 | avg_adv=0.0000\n",
      "[PPO] Iter 1 | rollout_reward=0.058886 | avg_reward=0.000230 | avg_return=0.026984 | avg_adv=0.0000\n",
      "[PPO] Iter 2 | rollout_reward=0.289213 | avg_reward=0.001130 | avg_return=0.068740 | avg_adv=0.0000\n",
      "[PPO] Iter 3 | rollout_reward=0.231293 | avg_reward=0.000903 | avg_return=0.066080 | avg_adv=0.0000\n",
      "[PPO] Iter 4 | rollout_reward=0.438638 | avg_reward=0.001713 | avg_return=0.057981 | avg_adv=-0.0000\n",
      "[PPO] Iter 5 | rollout_reward=0.083741 | avg_reward=0.000327 | avg_return=0.028280 | avg_adv=0.0000\n",
      "[PPO] Iter 6 | rollout_reward=0.130051 | avg_reward=0.000508 | avg_return=0.008649 | avg_adv=-0.0000\n",
      "[PPO] Iter 7 | rollout_reward=0.401650 | avg_reward=0.001569 | avg_return=0.139654 | avg_adv=0.0000\n",
      "[PPO] Iter 8 | rollout_reward=1.248102 | avg_reward=0.004875 | avg_return=0.391111 | avg_adv=-0.0000\n",
      "[PPO] Iter 9 | rollout_reward=0.308719 | avg_reward=0.001206 | avg_return=0.065174 | avg_adv=0.0000\n",
      "[PPO] Iter 10 | rollout_reward=-0.442118 | avg_reward=-0.001727 | avg_return=-0.103635 | avg_adv=0.0000\n",
      "[PPO] Iter 11 | rollout_reward=0.034424 | avg_reward=0.000134 | avg_return=0.020179 | avg_adv=-0.0000\n",
      "[PPO] Iter 12 | rollout_reward=0.217109 | avg_reward=0.000848 | avg_return=0.071834 | avg_adv=-0.0000\n",
      "[PPO] Iter 13 | rollout_reward=0.232272 | avg_reward=0.000907 | avg_return=0.030427 | avg_adv=0.0000\n",
      "[PPO] Iter 14 | rollout_reward=0.211618 | avg_reward=0.000827 | avg_return=0.034118 | avg_adv=0.0000\n",
      "Episódio 0 -> reward total = 3.59791\n",
      "[PPO] Iter 15 | rollout_reward=0.178296 | avg_reward=0.000696 | avg_return=0.012591 | avg_adv=0.0000\n",
      "[PPO] Iter 16 | rollout_reward=-0.040064 | avg_reward=-0.000156 | avg_return=-0.007781 | avg_adv=0.0000\n",
      "[PPO] Iter 17 | rollout_reward=0.295035 | avg_reward=0.001152 | avg_return=0.079972 | avg_adv=-0.0000\n",
      "[PPO] Iter 18 | rollout_reward=0.248089 | avg_reward=0.000969 | avg_return=0.066302 | avg_adv=-0.0000\n",
      "[PPO] Iter 19 | rollout_reward=0.655424 | avg_reward=0.002560 | avg_return=0.193868 | avg_adv=-0.0000\n",
      "[PPO] Iter 20 | rollout_reward=-0.176472 | avg_reward=-0.000689 | avg_return=-0.043839 | avg_adv=0.0000\n",
      "[PPO] Iter 21 | rollout_reward=0.165609 | avg_reward=0.000647 | avg_return=0.067864 | avg_adv=0.0000\n",
      "[PPO] Iter 22 | rollout_reward=-0.098244 | avg_reward=-0.000384 | avg_return=-0.027628 | avg_adv=0.0000\n",
      "[PPO] Iter 23 | rollout_reward=0.612813 | avg_reward=0.002394 | avg_return=0.143457 | avg_adv=0.0000\n",
      "[PPO] Iter 24 | rollout_reward=0.960710 | avg_reward=0.003753 | avg_return=0.197318 | avg_adv=-0.0000\n",
      "[PPO] Iter 25 | rollout_reward=-0.073156 | avg_reward=-0.000286 | avg_return=-0.040077 | avg_adv=-0.0000\n",
      "[PPO] Iter 26 | rollout_reward=-0.104404 | avg_reward=-0.000408 | avg_return=0.014246 | avg_adv=-0.0000\n",
      "[PPO] Iter 27 | rollout_reward=0.040700 | avg_reward=0.000159 | avg_return=0.006779 | avg_adv=-0.0000\n",
      "[PPO] Iter 28 | rollout_reward=0.457929 | avg_reward=0.001789 | avg_return=0.088225 | avg_adv=0.0000\n",
      "[PPO] Iter 29 | rollout_reward=0.120580 | avg_reward=0.000471 | avg_return=0.038351 | avg_adv=-0.0000\n",
      "[PPO] Iter 30 | rollout_reward=0.182299 | avg_reward=0.000712 | avg_return=0.071649 | avg_adv=-0.0000\n",
      "Episódio 1 -> reward total = 3.27095\n",
      "[PPO] Iter 31 | rollout_reward=-0.041812 | avg_reward=-0.000163 | avg_return=-0.007440 | avg_adv=0.0000\n",
      "[PPO] Iter 32 | rollout_reward=0.172938 | avg_reward=0.000676 | avg_return=0.056291 | avg_adv=0.0000\n",
      "[PPO] Iter 33 | rollout_reward=0.422475 | avg_reward=0.001650 | avg_return=0.119930 | avg_adv=0.0000\n",
      "[PPO] Iter 34 | rollout_reward=0.382779 | avg_reward=0.001495 | avg_return=0.104833 | avg_adv=-0.0000\n",
      "[PPO] Iter 35 | rollout_reward=0.293375 | avg_reward=0.001146 | avg_return=0.038872 | avg_adv=0.0000\n",
      "[PPO] Iter 36 | rollout_reward=-0.021595 | avg_reward=-0.000084 | avg_return=-0.011820 | avg_adv=-0.0000\n",
      "[PPO] Iter 37 | rollout_reward=0.243968 | avg_reward=0.000953 | avg_return=0.036953 | avg_adv=-0.0000\n",
      "[PPO] Iter 38 | rollout_reward=0.210097 | avg_reward=0.000821 | avg_return=0.079778 | avg_adv=0.0000\n",
      "[PPO] Iter 39 | rollout_reward=1.106611 | avg_reward=0.004323 | avg_return=0.299961 | avg_adv=0.0000\n",
      "[PPO] Iter 40 | rollout_reward=0.241352 | avg_reward=0.000943 | avg_return=0.052044 | avg_adv=-0.0000\n",
      "[PPO] Iter 41 | rollout_reward=-0.337963 | avg_reward=-0.001320 | avg_return=-0.096779 | avg_adv=-0.0000\n",
      "[PPO] Iter 42 | rollout_reward=0.074911 | avg_reward=0.000293 | avg_return=0.034837 | avg_adv=-0.0000\n",
      "[PPO] Iter 43 | rollout_reward=0.278091 | avg_reward=0.001086 | avg_return=0.073994 | avg_adv=0.0000\n",
      "[PPO] Iter 44 | rollout_reward=0.254734 | avg_reward=0.000995 | avg_return=0.029119 | avg_adv=-0.0000\n",
      "[PPO] Iter 45 | rollout_reward=0.152031 | avg_reward=0.000594 | avg_return=0.019956 | avg_adv=0.0000\n",
      "Episódio 2 -> reward total = 3.57072\n",
      "[PPO] Iter 46 | rollout_reward=0.138610 | avg_reward=0.000541 | avg_return=0.009439 | avg_adv=-0.0000\n",
      "[PPO] Iter 47 | rollout_reward=0.032486 | avg_reward=0.000127 | avg_return=0.008399 | avg_adv=0.0000\n",
      "[PPO] Iter 48 | rollout_reward=0.267086 | avg_reward=0.001043 | avg_return=0.091439 | avg_adv=0.0000\n",
      "[PPO] Iter 49 | rollout_reward=0.449446 | avg_reward=0.001756 | avg_return=0.129856 | avg_adv=0.0000\n"
     ]
    }
   ],
   "source": [
    "# ? Loop de treino\n",
    "num_iterations = 50   # iterações de PPO (cada uma com 1 rollout + vários updates)\n",
    "episodio_global = 0   # contador de episódios (igual A2C)\n",
    "\n",
    "# reset só uma vez, fora do loop\n",
    "state = env.reset()\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "ep_reward = 0.0  # recompensa acumulada do episódio atual\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    states_buf = []\n",
    "    actions_buf = []\n",
    "    logprobs_buf = []\n",
    "    rewards_buf = []\n",
    "    dones_buf = []\n",
    "    values_buf = []\n",
    "\n",
    "    # 1) COLETA DE TRAJETÓRIA (ROLLOUT)\n",
    "    for step in range(rollout_len):\n",
    "        # policy -> logits\n",
    "        logits = policy(state)\n",
    "\n",
    "        if torch.isnan(logits).any() or not torch.isfinite(logits).all():\n",
    "            print(\"DEU RUIM: logits inválidos (NaN/Inf)\")\n",
    "            print(\"state:\", state)\n",
    "            print(\"logits:\", logits)\n",
    "            raise RuntimeError(\"Logits NaN/Inf em PPO\")\n",
    "\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        # value atual\n",
    "        V = value(state)\n",
    "\n",
    "        # aplica ação -> novos pesos\n",
    "        novos_pesos = aplicar_acao_portfolio(env.pesos, action.item())\n",
    "\n",
    "        # passo no ambiente\n",
    "        next_state, reward, done = env.step(novos_pesos)\n",
    "\n",
    "        # acumula recompensa do episódio (para log tipo A2C)\n",
    "        ep_reward += reward\n",
    "\n",
    "        # guarda no buffer (em numpy, sem batch)\n",
    "        states_buf.append(state.squeeze(0).detach().numpy())\n",
    "        actions_buf.append(action.item())\n",
    "        logprobs_buf.append(log_prob.item())\n",
    "        rewards_buf.append(reward)\n",
    "        dones_buf.append(float(done))\n",
    "        values_buf.append(V.item())\n",
    "\n",
    "        # prepara próximo estado\n",
    "        state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            # log igual ao A2C\n",
    "            print(f\"Episódio {episodio_global} -> reward total = {ep_reward:.5f}\")\n",
    "            episodio_global += 1\n",
    "            ep_reward = 0.0  # zera para o próximo episódio\n",
    "\n",
    "            # começa novo episódio\n",
    "            state = env.reset()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    # 2) CONVERTE BUFFER EM TENSORES\n",
    "    states = torch.FloatTensor(np.array(states_buf))            # [T, state_dim]\n",
    "    actions = torch.LongTensor(np.array(actions_buf))           # [T]\n",
    "    old_logprobs = torch.FloatTensor(np.array(logprobs_buf))    # [T]\n",
    "    rewards = np.array(rewards_buf, dtype=np.float32)           # [T]\n",
    "    dones = np.array(dones_buf, dtype=np.float32)               # [T]\n",
    "    values_arr = np.array(values_buf, dtype=np.float32)         # [T]\n",
    "\n",
    "    # 3) RETURNS + ADVANTAGES (sem GAE)\n",
    "    returns_arr, advantages_arr = compute_returns_and_advantages(\n",
    "        rewards, dones, values_arr, gamma=gamma\n",
    "    )\n",
    "\n",
    "    returns = torch.FloatTensor(returns_arr)\n",
    "    advantages = torch.FloatTensor(advantages_arr)\n",
    "\n",
    "    # 4) PPO UPDATES (várias epochs sobre a mesma trajetória)\n",
    "    dataset_size = states.size(0)\n",
    "    idxs = np.arange(dataset_size)\n",
    "\n",
    "    for epoch in range(ppo_epochs):\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        for start in range(0, dataset_size, batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_idx = idxs[start:end]\n",
    "\n",
    "            batch_states = states[batch_idx]\n",
    "            batch_actions = actions[batch_idx]\n",
    "            batch_old_logprobs = old_logprobs[batch_idx]\n",
    "            batch_returns = returns[batch_idx]\n",
    "            batch_advantages = advantages[batch_idx]\n",
    "\n",
    "            # policy nova (π_new)\n",
    "            logits_new = policy(batch_states)\n",
    "            dist_new = Categorical(logits=logits_new)\n",
    "            new_logprobs = dist_new.log_prob(batch_actions)\n",
    "            entropy = dist_new.entropy().mean()\n",
    "\n",
    "            # ratio = π_new / π_old\n",
    "            ratios = torch.exp(new_logprobs - batch_old_logprobs)\n",
    "\n",
    "            surr1 = ratios * batch_advantages\n",
    "            surr2 = torch.clamp(ratios, 1.0 - clip_eps, 1.0 + clip_eps) * batch_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            # value loss (MSE entre V e returns)\n",
    "            V_pred = value(batch_states).squeeze(-1)\n",
    "            value_loss = nn.functional.mse_loss(V_pred, batch_returns)\n",
    "\n",
    "            # perda total\n",
    "            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "            optimizerP.zero_grad()\n",
    "            optimizerV.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizerP.step()\n",
    "            optimizerV.step()\n",
    "\n",
    "    # 5) LOG BÁSICO (nível de rollout/iteração)\n",
    "    # Se você quiser ficar mais parecido com o A2C, pode logar a soma do rollout:\n",
    "    total_rollout_reward = rewards.sum()\n",
    "    avg_reward = rewards.mean()\n",
    "    avg_return = returns.mean().item()\n",
    "    avg_adv = advantages.mean().item()\n",
    "\n",
    "    print(f\"[PPO] Iter {it} | \"\n",
    "          f\"rollout_reward={total_rollout_reward:.6f} | \"\n",
    "          f\"avg_reward={avg_reward:.6f} | \"\n",
    "          f\"avg_return={avg_return:.6f} | \"\n",
    "          f\"avg_adv={avg_adv:.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7101abf3",
   "metadata": {},
   "source": [
    "PPO + GEN Semana 3 Modelo para Produção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b2cb3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo encontrado! Carregando dados locais...\n",
      "            BOVA11.SA  SMAL11.SA  IVVB11.SA  PETR4.SA   VALE3.SA  ITUB4.SA  \\\n",
      "Date                                                                         \n",
      "2015-01-01        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-02  47.259998  52.020000  55.799999  2.572509  10.505502  9.372999   \n",
      "2015-01-03        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-04        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-05  46.320000  50.549999  55.750000  2.352637  10.347521  9.420101   \n",
      "\n",
      "                   SPY        QQQ         IWM        EEM         GLD  \\\n",
      "Date                                                                   \n",
      "2015-01-01         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-02  171.093704  94.906532  103.315140  30.789457  114.080002   \n",
      "2015-01-03         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-04         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-05  168.003784  93.514397  101.933861  30.241489  115.800003   \n",
      "\n",
      "                  TLT     BTC-USD  ETH-USD  BNB-USD  SOL-USD  ADA-USD  \n",
      "Date                                                                   \n",
      "2015-01-01        NaN  314.248993      NaN      NaN      NaN      NaN  \n",
      "2015-01-02  95.468956  315.032013      NaN      NaN      NaN      NaN  \n",
      "2015-01-03        NaN  281.082001      NaN      NaN      NaN      NaN  \n",
      "2015-01-04        NaN  264.195007      NaN      NaN      NaN      NaN  \n",
      "2015-01-05  96.968613  274.473999      NaN      NaN      NaN      NaN  \n",
      "(3977, 17)\n",
      "NUM_ATIVOS = 17\n",
      "-0.008030805295683797\n",
      "Soma: 1.0000000000000002\n",
      "Min: 0.05767012687427913\n",
      "Max: 0.07727797001153404\n",
      "[0.07727797 0.05767013 0.05767013 0.05767013 0.05767013]\n",
      "Ação 0 | soma=1.000000 | min=0.0577 | max=0.0773\n",
      "Ação 1 | soma=1.000000 | min=0.0565 | max=0.0761\n",
      "Ação 2 | soma=1.000000 | min=0.0554 | max=0.0750\n",
      "Ação 3 | soma=1.000000 | min=0.0543 | max=0.0740\n",
      "Ação 4 | soma=1.000000 | min=0.0533 | max=0.0729\n",
      "Ação 5 | soma=1.000000 | min=0.0522 | max=0.0718\n",
      "Ação 6 | soma=1.000000 | min=0.0512 | max=0.0708\n",
      "Ação 7 | soma=1.000000 | min=0.0502 | max=0.0698\n",
      "Ação 8 | soma=1.000000 | min=0.0492 | max=0.0688\n",
      "Ação 9 | soma=1.000000 | min=0.0483 | max=0.0679\n",
      "Arquivo encontrado! Carregando dados locais...\n",
      "            BOVA11.SA  SMAL11.SA  IVVB11.SA  PETR4.SA   VALE3.SA  ITUB4.SA  \\\n",
      "Date                                                                         \n",
      "2015-01-01        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-02  47.259998  52.020000  55.799999  2.572509  10.505502  9.372999   \n",
      "2015-01-03        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-04        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-05  46.320000  50.549999  55.750000  2.352637  10.347521  9.420101   \n",
      "\n",
      "                   SPY        QQQ         IWM        EEM         GLD  \\\n",
      "Date                                                                   \n",
      "2015-01-01         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-02  171.093704  94.906532  103.315140  30.789457  114.080002   \n",
      "2015-01-03         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-04         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-05  168.003784  93.514397  101.933861  30.241489  115.800003   \n",
      "\n",
      "                  TLT     BTC-USD  ETH-USD  BNB-USD  SOL-USD  ADA-USD  \n",
      "Date                                                                   \n",
      "2015-01-01        NaN  314.248993      NaN      NaN      NaN      NaN  \n",
      "2015-01-02  95.468956  315.032013      NaN      NaN      NaN      NaN  \n",
      "2015-01-03        NaN  281.082001      NaN      NaN      NaN      NaN  \n",
      "2015-01-04        NaN  264.195007      NaN      NaN      NaN      NaN  \n",
      "2015-01-05  96.968613  274.473999      NaN      NaN      NaN      NaN  \n",
      "(3977, 17)\n",
      "NUM_ATIVOS = 17\n",
      "-0.008030805295683797\n",
      "Soma: 1.0000000000000002\n",
      "Min: 0.05767012687427913\n",
      "Max: 0.07727797001153404\n",
      "[0.07727797 0.05767013 0.05767013 0.05767013 0.05767013]\n",
      "Ação 0 | soma=1.000000 | min=0.0577 | max=0.0773\n",
      "Ação 1 | soma=1.000000 | min=0.0565 | max=0.0761\n",
      "Ação 2 | soma=1.000000 | min=0.0554 | max=0.0750\n",
      "Ação 3 | soma=1.000000 | min=0.0543 | max=0.0740\n",
      "Ação 4 | soma=1.000000 | min=0.0533 | max=0.0729\n",
      "Ação 5 | soma=1.000000 | min=0.0522 | max=0.0718\n",
      "Ação 6 | soma=1.000000 | min=0.0512 | max=0.0708\n",
      "Ação 7 | soma=1.000000 | min=0.0502 | max=0.0698\n",
      "Ação 8 | soma=1.000000 | min=0.0492 | max=0.0688\n",
      "Ação 9 | soma=1.000000 | min=0.0483 | max=0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olazu\\OneDrive\\Documentos\\Python\\portfolioDeAtivos\\portfolioDeAtivos\\teste.py:154: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  retornos = precos.pct_change().fillna(0)\n",
      "c:\\Users\\olazu\\OneDrive\\Documentos\\Python\\portfolioDeAtivos\\portfolioDeAtivos\\teste.py:154: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  retornos = precos.pct_change().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO] Iter 0 | rollout_reward=-0.020023 | avg_reward=-0.000078 | avg_return=0.499928 | avg_adv=0.0000\n",
      "[PPO] Iter 1 | rollout_reward=0.044820 | avg_reward=0.000175 | avg_return=0.440698 | avg_adv=0.0000\n",
      "[PPO] Iter 2 | rollout_reward=0.260921 | avg_reward=0.001019 | avg_return=0.374021 | avg_adv=-0.0000\n",
      "[PPO] Iter 3 | rollout_reward=0.152864 | avg_reward=0.000597 | avg_return=0.316918 | avg_adv=0.0000\n",
      "[PPO] Iter 4 | rollout_reward=0.124182 | avg_reward=0.000485 | avg_return=0.302382 | avg_adv=-0.0000\n",
      "[PPO] Iter 5 | rollout_reward=0.047876 | avg_reward=0.000187 | avg_return=0.239331 | avg_adv=-0.0000\n",
      "[PPO] Iter 6 | rollout_reward=0.101755 | avg_reward=0.000397 | avg_return=0.214473 | avg_adv=-0.0000\n",
      "[PPO] Iter 7 | rollout_reward=0.196017 | avg_reward=0.000766 | avg_return=0.199145 | avg_adv=0.0000\n",
      "[PPO] Iter 8 | rollout_reward=1.168630 | avg_reward=0.004565 | avg_return=0.229831 | avg_adv=-0.0000\n",
      "[PPO] Iter 9 | rollout_reward=0.402589 | avg_reward=0.001573 | avg_return=0.204001 | avg_adv=0.0000\n",
      "[PPO] Iter 10 | rollout_reward=-0.792325 | avg_reward=-0.003095 | avg_return=0.128988 | avg_adv=0.0000\n",
      "[PPO] Iter 11 | rollout_reward=-0.657405 | avg_reward=-0.002568 | avg_return=0.076428 | avg_adv=-0.0000\n",
      "Episódio 0 -> reward total = 0.90906\n",
      "[PPO] Iter 12 | rollout_reward=-0.065524 | avg_reward=-0.000256 | avg_return=0.035133 | avg_adv=0.0000\n",
      "[PPO] Iter 13 | rollout_reward=-0.215423 | avg_reward=-0.000841 | avg_return=0.015598 | avg_adv=0.0000\n",
      "[PPO] Iter 14 | rollout_reward=0.403430 | avg_reward=0.001576 | avg_return=0.046826 | avg_adv=0.0000\n",
      "[PPO] Iter 15 | rollout_reward=0.104955 | avg_reward=0.000410 | avg_return=0.030976 | avg_adv=-0.0000\n",
      "[PPO] Iter 16 | rollout_reward=0.327772 | avg_reward=0.001280 | avg_return=0.051681 | avg_adv=0.0000\n",
      "[PPO] Iter 17 | rollout_reward=-0.036844 | avg_reward=-0.000144 | avg_return=0.048322 | avg_adv=0.0000\n",
      "[PPO] Iter 18 | rollout_reward=0.075666 | avg_reward=0.000296 | avg_return=0.046647 | avg_adv=-0.0000\n",
      "[PPO] Iter 19 | rollout_reward=-0.219944 | avg_reward=-0.000859 | avg_return=0.031693 | avg_adv=0.0000\n",
      "[PPO] Iter 20 | rollout_reward=0.827071 | avg_reward=0.003231 | avg_return=0.083335 | avg_adv=0.0000\n",
      "[PPO] Iter 21 | rollout_reward=1.030866 | avg_reward=0.004027 | avg_return=0.120554 | avg_adv=-0.0000\n",
      "[PPO] Iter 22 | rollout_reward=-0.329857 | avg_reward=-0.001289 | avg_return=0.068780 | avg_adv=-0.0000\n",
      "[PPO] Iter 23 | rollout_reward=-0.882945 | avg_reward=-0.003449 | avg_return=0.009358 | avg_adv=0.0000\n",
      "Episódio 1 -> reward total = 0.75416\n",
      "[PPO] Iter 24 | rollout_reward=-0.395421 | avg_reward=-0.001545 | avg_return=-0.009334 | avg_adv=-0.0000\n",
      "[PPO] Iter 25 | rollout_reward=-0.009799 | avg_reward=-0.000038 | avg_return=-0.017156 | avg_adv=-0.0000\n",
      "[PPO] Iter 26 | rollout_reward=0.038328 | avg_reward=0.000150 | avg_return=-0.008731 | avg_adv=-0.0000\n",
      "[PPO] Iter 27 | rollout_reward=0.147165 | avg_reward=0.000575 | avg_return=0.004288 | avg_adv=-0.0000\n",
      "[PPO] Iter 28 | rollout_reward=0.210008 | avg_reward=0.000820 | avg_return=0.015001 | avg_adv=-0.0000\n",
      "[PPO] Iter 29 | rollout_reward=0.061535 | avg_reward=0.000240 | avg_return=0.011628 | avg_adv=0.0000\n",
      "[PPO] Iter 30 | rollout_reward=0.041238 | avg_reward=0.000161 | avg_return=0.003392 | avg_adv=-0.0000\n",
      "[PPO] Iter 31 | rollout_reward=0.074106 | avg_reward=0.000289 | avg_return=0.021319 | avg_adv=-0.0000\n",
      "[PPO] Iter 32 | rollout_reward=0.320837 | avg_reward=0.001253 | avg_return=0.046695 | avg_adv=0.0000\n",
      "[PPO] Iter 33 | rollout_reward=0.934236 | avg_reward=0.003649 | avg_return=0.099527 | avg_adv=-0.0000\n",
      "[PPO] Iter 34 | rollout_reward=0.235915 | avg_reward=0.000922 | avg_return=0.060960 | avg_adv=0.0000\n",
      "[PPO] Iter 35 | rollout_reward=-0.859949 | avg_reward=-0.003359 | avg_return=-0.006577 | avg_adv=-0.0000\n",
      "[PPO] Iter 36 | rollout_reward=-0.771174 | avg_reward=-0.003012 | avg_return=-0.025849 | avg_adv=-0.0000\n",
      "Episódio 2 -> reward total = 0.24687\n",
      "[PPO] Iter 37 | rollout_reward=-0.149179 | avg_reward=-0.000583 | avg_return=-0.027178 | avg_adv=0.0000\n",
      "[PPO] Iter 38 | rollout_reward=0.030511 | avg_reward=0.000119 | avg_return=-0.019160 | avg_adv=0.0000\n",
      "[PPO] Iter 39 | rollout_reward=0.136956 | avg_reward=0.000535 | avg_return=-0.008227 | avg_adv=-0.0000\n",
      "[PPO] Iter 40 | rollout_reward=0.168602 | avg_reward=0.000659 | avg_return=0.002184 | avg_adv=0.0000\n",
      "[PPO] Iter 41 | rollout_reward=0.358098 | avg_reward=0.001399 | avg_return=0.028071 | avg_adv=0.0000\n",
      "[PPO] Iter 42 | rollout_reward=-0.022057 | avg_reward=-0.000086 | avg_return=0.011399 | avg_adv=0.0000\n",
      "[PPO] Iter 43 | rollout_reward=0.170965 | avg_reward=0.000668 | avg_return=0.021513 | avg_adv=0.0000\n",
      "[PPO] Iter 44 | rollout_reward=-0.142365 | avg_reward=-0.000556 | avg_return=0.009196 | avg_adv=0.0000\n",
      "[PPO] Iter 45 | rollout_reward=0.970007 | avg_reward=0.003789 | avg_return=0.079627 | avg_adv=-0.0000\n",
      "[PPO] Iter 46 | rollout_reward=0.826287 | avg_reward=0.003228 | avg_return=0.094622 | avg_adv=-0.0000\n",
      "[PPO] Iter 47 | rollout_reward=-0.858426 | avg_reward=-0.003353 | avg_return=-0.002643 | avg_adv=-0.0000\n",
      "[PPO] Iter 48 | rollout_reward=-1.019780 | avg_reward=-0.003984 | avg_return=-0.045396 | avg_adv=0.0000\n",
      "Episódio 3 -> reward total = 0.14702\n",
      "[PPO] Iter 49 | rollout_reward=-0.455683 | avg_reward=-0.001780 | avg_return=-0.044439 | avg_adv=0.0000\n",
      "\n",
      "===== AVALIAÇÃO NO TESTE (REWARD FULL) =====\n",
      "Total reward (teste) = 0.807610\n",
      "Média diária (teste) = 0.001016\n",
      "N dias de teste = 795\n",
      "\n",
      "===== AVALIAÇÃO NO TESTE (RETORNO PURO) =====\n",
      "Total log-ret puro = 0.888618\n",
      "Média diária log-ret puro = 0.001118\n",
      "N dias de teste = 795\n"
     ]
    }
   ],
   "source": [
    "# PPO + GEN Semana 3 Modelo para Produção\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "import random\n",
    "import importlib\n",
    "import teste  # importa o módulo\n",
    "importlib.reload(teste)\n",
    "from rl_env import PortfolioEnv, PolicyMLP, ValueMLP\n",
    "from teste import (\n",
    "    ret_train, r_media_train, v_media_train, dd_train, regime_ids_train,\n",
    "    ret_test, r_media_test, v_media_test, dd_test, regime_ids_test,\n",
    "    cluster_ids, NUM_ATIVOS,\n",
    "    aplicar_acao_portfolio\n",
    ")\n",
    "\n",
    "SEED = 5\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Para deixar o PyTorch mais determinístico:\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# =========================\n",
    "# Hiperparâmetros\n",
    "# =========================\n",
    "\n",
    "AÇÕES_POR_ATIVOS = 2\n",
    "num_actions = NUM_ATIVOS * AÇÕES_POR_ATIVOS\n",
    "\n",
    "state_dim = NUM_ATIVOS + 3 + NUM_ATIVOS  # pesos + (r_media, v_media, dd) + clusters\n",
    "\n",
    "gamma = 0.99       # desconto das recompensas futuras\n",
    "clip_eps = 0.2     # clipping do PPO\n",
    "ppo_epochs = 4     # quantas vezes reaproveitar o mesmo rollout\n",
    "rollout_len = 256  # passos por iteração de PPO\n",
    "batch_size = 64    # tamanho do minibatch\n",
    "lr = 1e-4\n",
    "entropy_coef = 0.01\n",
    "value_coef = 0.5\n",
    "\n",
    "# =========================\n",
    "# Modelos\n",
    "# =========================\n",
    "policy = PolicyMLP(state_dim, num_actions)\n",
    "value = ValueMLP(state_dim)\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "policy.apply(init_weights)\n",
    "value.apply(init_weights)\n",
    "\n",
    "optimizerP = optim.Adam(policy.parameters(), lr=lr)\n",
    "optimizerV = optim.Adam(value.parameters(), lr=lr)\n",
    "\n",
    "# =========================\n",
    "# Ambiente de TREINO (só dados de treino)\n",
    "# =========================\n",
    "env_train = PortfolioEnv(\n",
    "    ret_train,\n",
    "    r_media_train,\n",
    "    v_media_train,\n",
    "    dd_train,\n",
    "    regime_ids_train,\n",
    "    cluster_ids\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# Função auxiliar: GAE\n",
    "# =========================\n",
    "def compute_gae(rewards, values, dones, last_value, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    GAE(λ) - Generalized Advantage Estimation\n",
    "    \"\"\"\n",
    "    rewards = np.array(rewards, dtype=np.float32)\n",
    "    values = np.array(values, dtype=np.float32)\n",
    "    dones = np.array(dones, dtype=np.float32)\n",
    "\n",
    "    values_ext = np.append(values, last_value)\n",
    "\n",
    "    T = len(rewards)\n",
    "    advantages = np.zeros(T, dtype=np.float32)\n",
    "    gae = 0.0\n",
    "\n",
    "    for t in reversed(range(T)):\n",
    "        delta = rewards[t] + gamma * values_ext[t + 1] * (1.0 - dones[t]) - values_ext[t]\n",
    "        gae = delta + gamma * lam * (1.0 - dones[t]) * gae\n",
    "        advantages[t] = gae\n",
    "\n",
    "    returns = advantages + values\n",
    "\n",
    "    adv_mean = advantages.mean()\n",
    "    adv_std = advantages.std() + 1e-8\n",
    "    advantages = (advantages - adv_mean) / adv_std\n",
    "\n",
    "    return returns, advantages\n",
    "\n",
    "\n",
    "# =========================\n",
    "# Loop de TREINO (PPO + GAE) – só no TREINO\n",
    "# =========================\n",
    "num_iterations = 50   # iterações de PPO (cada uma com 1 rollout + vários updates)\n",
    "episodio_global = 0\n",
    "\n",
    "# reset só uma vez, fora do loop, no ambiente de treino\n",
    "state = env_train.reset()\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "ep_reward = 0.0\n",
    "\n",
    "for it in range(num_iterations):\n",
    "    states_buf = []\n",
    "    actions_buf = []\n",
    "    logprobs_buf = []\n",
    "    rewards_buf = []\n",
    "    dones_buf = []\n",
    "    values_buf = []\n",
    "\n",
    "    # 1) COLETA DE TRAJETÓRIA (ROLLOUT) no ENV_TRAIN\n",
    "    for step in range(rollout_len):\n",
    "        logits = policy(state)\n",
    "\n",
    "        if torch.isnan(logits).any() or not torch.isfinite(logits).all():\n",
    "            print(\"DEU RUIM: logits inválidos (NaN/Inf)\")\n",
    "            print(\"state:\", state)\n",
    "            print(\"logits:\", logits)\n",
    "            raise RuntimeError(\"Logits NaN/Inf em PPO\")\n",
    "\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "\n",
    "        V = value(state)\n",
    "\n",
    "        # aplica ação -> novos pesos usando o ambiente de TREINO\n",
    "        novos_pesos = aplicar_acao_portfolio(env_train.pesos, action.item())\n",
    "\n",
    "        next_state, reward, done = env_train.step(novos_pesos)\n",
    "\n",
    "        ep_reward += reward\n",
    "\n",
    "        states_buf.append(state.squeeze(0).detach().numpy())\n",
    "        actions_buf.append(action.item())\n",
    "        logprobs_buf.append(log_prob.item())\n",
    "        rewards_buf.append(reward)\n",
    "        dones_buf.append(float(done))\n",
    "        values_buf.append(V.item())\n",
    "\n",
    "        state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episódio {episodio_global} -> reward total = {ep_reward:.5f}\")\n",
    "            episodio_global += 1\n",
    "            ep_reward = 0.0\n",
    "\n",
    "            state = env_train.reset()\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    # 2) CONVERTE BUFFER EM TENSORES / ARRAYS\n",
    "    states = torch.FloatTensor(np.array(states_buf))\n",
    "    actions = torch.LongTensor(np.array(actions_buf))\n",
    "    old_logprobs = torch.FloatTensor(np.array(logprobs_buf))\n",
    "    rewards = np.array(rewards_buf, dtype=np.float32)\n",
    "    dones = np.array(dones_buf, dtype=np.float32)\n",
    "    values_arr = np.array(values_buf, dtype=np.float32)\n",
    "\n",
    "    # pega V(s_{T+1}) com o último state do rollout\n",
    "    with torch.no_grad():\n",
    "        last_value = value(state).item()\n",
    "\n",
    "    # 3) RETURNS + ADVANTAGES com GAE\n",
    "    returns_arr, advantages_arr = compute_gae(\n",
    "        rewards, values_arr, dones, last_value, gamma=gamma, lam=0.95\n",
    "    )\n",
    "\n",
    "    returns = torch.FloatTensor(returns_arr)\n",
    "    advantages = torch.FloatTensor(advantages_arr)\n",
    "\n",
    "    # 4) PPO UPDATES\n",
    "    dataset_size = states.size(0)\n",
    "    idxs = np.arange(dataset_size)\n",
    "\n",
    "    for epoch in range(ppo_epochs):\n",
    "        np.random.shuffle(idxs)\n",
    "\n",
    "        for start in range(0, dataset_size, batch_size):\n",
    "            end = start + batch_size\n",
    "            batch_idx = idxs[start:end]\n",
    "\n",
    "            batch_states = states[batch_idx]\n",
    "            batch_actions = actions[batch_idx]\n",
    "            batch_old_logprobs = old_logprobs[batch_idx]\n",
    "            batch_returns = returns[batch_idx]\n",
    "            batch_advantages = advantages[batch_idx]\n",
    "\n",
    "            logits_new = policy(batch_states)\n",
    "            dist_new = Categorical(logits=logits_new)\n",
    "            new_logprobs = dist_new.log_prob(batch_actions)\n",
    "            entropy = dist_new.entropy().mean()\n",
    "\n",
    "            ratios = torch.exp(new_logprobs - batch_old_logprobs)\n",
    "\n",
    "            surr1 = ratios * batch_advantages\n",
    "            surr2 = torch.clamp(ratios, 1.0 - clip_eps, 1.0 + clip_eps) * batch_advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "\n",
    "            V_pred = value(batch_states).squeeze(-1)\n",
    "            value_loss = nn.functional.mse_loss(V_pred, batch_returns)\n",
    "\n",
    "            loss = policy_loss + value_coef * value_loss - entropy_coef * entropy\n",
    "\n",
    "            optimizerP.zero_grad()\n",
    "            optimizerV.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizerP.step()\n",
    "            optimizerV.step()\n",
    "\n",
    "    total_rollout_reward = rewards.sum()\n",
    "    avg_reward = rewards.mean()\n",
    "    avg_return = returns.mean().item()\n",
    "    avg_adv = advantages.mean().item()\n",
    "\n",
    "    print(f\"[PPO] Iter {it} | \"\n",
    "          f\"rollout_reward={total_rollout_reward:.6f} | \"\n",
    "          f\"avg_reward={avg_reward:.6f} | \"\n",
    "          f\"avg_return={avg_return:.6f} | \"\n",
    "          f\"avg_adv={avg_adv:.4f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# AVALIAÇÃO NO CONJUNTO DE TESTE (sem treinar!)\n",
    "# =========================\n",
    "\n",
    "env_test = PortfolioEnv(\n",
    "    ret_test,\n",
    "    r_media_test,\n",
    "    v_media_test,\n",
    "    dd_test,\n",
    "    regime_ids_test,\n",
    "    cluster_ids\n",
    ")\n",
    "\n",
    "state = env_test.reset()\n",
    "state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "# ===== AVALIAÇÃO NO TESTE: REWARD FULL + RETORNO PURO =====\n",
    "portfolio_log_ret = []        # reward completo (logret - dd - custo)\n",
    "portfolio_log_ret_puro = []   # logret puro (sem penalização)\n",
    "\n",
    "with torch.no_grad():\n",
    "    done = False\n",
    "    while not done:\n",
    "        # 1) política escolhe ação\n",
    "        logits = policy(state)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "\n",
    "        # 2) aplica ação e avança o ambiente de teste\n",
    "        next_state, reward, done = env_test.step(\n",
    "            aplicar_acao_portfolio(env_test.pesos, action.item())\n",
    "        )\n",
    "\n",
    "        # 3) computa log-retorno PURO (sem dd, sem custo)\n",
    "        r_t = ret_test.iloc[env_test.t - 1].values  # retornos do dia atual\n",
    "        r_p = float(np.dot(env_test.pesos, r_t))\n",
    "\n",
    "        if 1 + r_p <= 0:\n",
    "            log_ret_puro = -10.0\n",
    "        else:\n",
    "            log_ret_puro = np.log(1 + r_p)\n",
    "\n",
    "        # 4) salva as métricas\n",
    "        portfolio_log_ret.append(reward)            # reward FULL\n",
    "        portfolio_log_ret_puro.append(log_ret_puro) # só log-ret\n",
    "\n",
    "        # 5) atualiza o estado\n",
    "        state = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "total_reward_test = np.sum(portfolio_log_ret)\n",
    "avg_reward_test = np.mean(portfolio_log_ret)\n",
    "\n",
    "print(\"\\n===== AVALIAÇÃO NO TESTE (REWARD FULL) =====\")\n",
    "print(f\"Total reward (teste) = {total_reward_test:.6f}\")\n",
    "print(f\"Média diária (teste) = {avg_reward_test:.6f}\")\n",
    "print(f\"N dias de teste = {len(portfolio_log_ret)}\")\n",
    "\n",
    "print(\"\\n===== AVALIAÇÃO NO TESTE (RETORNO PURO) =====\")\n",
    "print(f\"Total log-ret puro = {np.sum(portfolio_log_ret_puro):.6f}\")\n",
    "print(f\"Média diária log-ret puro = {np.mean(portfolio_log_ret_puro):.6f}\")\n",
    "print(f\"N dias de teste = {len(portfolio_log_ret_puro)}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
