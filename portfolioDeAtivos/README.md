# ğŸ§  Sistema de Aprendizado por ReforÃ§o para AlocaÃ§Ã£o de PortfÃ³lio  
### A2C â†’ PPO â†’ PPO + GEN (GAE) â€¢ Pipeline completo de Treino/Teste
## âš ï¸ Aviso Importante

Antes de executar qualquer parte do projeto, **todos os arquivos `.ipynb` devem ser convertidos para o formato `.py`**.

O sistema **nÃ£o funcionarÃ¡ corretamente** enquanto houver notebooks sendo usados diretamente.

Para converter, utilize:

```sh
jupyter nbconvert --to script arquivo.ipynb


## ğŸ“¦ DescriÃ§Ã£o

Este projeto implementa um **sistema completo de Aprendizado por ReforÃ§o (RL)** para otimizaÃ§Ã£o de portfÃ³lios financeiros.  
Ele segue uma arquitetura modular dividida em:

- Core de dados e lÃ³gica financeira  
- Ambiente de simulaÃ§Ã£o estilo OpenAI Gym  
- Modelos Actorâ€“Critic  
- Treinos A2C, PPO e PPO+GEN (GAE)  
- AvaliaÃ§Ã£o separada em dataset de teste  

O objetivo Ã© permitir que o agente aprenda a:

- ğŸ“ˆ Ajustar pesos do portfÃ³lio  
- ğŸ“‰ Minimizar drawdown  
- ğŸ” Controlar turnover (custo de transaÃ§Ã£o)  
- ğŸ¯ Maximizar retorno logarÃ­tmico ajustado a risco  

Tudo isso utilizando dados histÃ³ricos reais e interaÃ§Ã£o ambienteâ€“agente.

---

# ğŸ“ Estrutura do Projeto

```
ğŸ“‚ RL-PORTFOLIO
â”œâ”€â”€ ğŸ“„ teste.py           â†’ NÃºcleo de dados e estatÃ­sticas (Core)
â”œâ”€â”€ ğŸ“„ rl_env.py          â†’ Ambiente e redes neurais (Simulador)
â”œâ”€â”€ ğŸ“„ train_a2c.py       â†’ Semana 1 â€¢ A2C para debug e validaÃ§Ã£o
â”œâ”€â”€ ğŸ“„ train_ppo.py       â†’ Semana 2 e 3 â€¢ PPO e PPO+GAE (PPO+GEN)
â”œâ”€â”€ ğŸ“„ app.py             â†’ Sandbox / utilidades / carregamento de dados
â”œâ”€â”€ ğŸ“„ README.md          â†’ DocumentaÃ§Ã£o principal
â””â”€â”€ ğŸ“„ requirements.txt   â†’ DependÃªncias
```

---

# ğŸ§© VisÃ£o Geral do Fluxo

Abaixo estÃ£o os locais destinados Ã s **trÃªs imagens** explicando o fluxo completo do projeto.

---

## ğŸ–¼ï¸ 1ï¸âƒ£ Diagrama Geral do Pipeline (Treino + Teste)

![Diagrama geral](./src/Untitled diagram-2025-11-29-184940.svg)

---

## ğŸ–¼ï¸ 2ï¸âƒ£ Fluxo do A2C (Semana 1 â€“ Debug)

![Fluxo A2C](./src/Gemini_Generated_Image_z9dogyz9dogyz9do.png)

---

## ğŸ–¼ï¸ 3ï¸âƒ£ Fluxo do Ambiente PortfolioEnv

![Fluxo do ambiente](./src/Untitled diagram-2025-11-29-185830.png)

---

# ğŸ› ï¸ Tecnologias Utilizadas

- Python  
- NumPy & Pandas  
- PyTorch  
- Yahoo Finance API (yfinance)  
- Reinforcement Learning (A2C / PPO / PPO+GAE)  
- Modelos Actorâ€“Critic  
- NormalizaÃ§Ã£o e seguranÃ§a numÃ©rica (tratamento de NaNs e infinitos)

---

# âš™ï¸ InstalaÃ§Ã£o

```sh
git clone <URL_DO_REPOSITORIO>
cd RL-PORTFOLIO
pip install -r requirements.txt
```

---

# ğŸ” ExplicaÃ§Ã£o dos Arquivos Principais

---

# ğŸ§© `teste.py` â€” O â€œCoreâ€: Dados, estatÃ­sticas e prÃ©-processamento

Este arquivo realiza:

### âœ” Download e atualizaÃ§Ã£o dos dados histÃ³ricos (Yahoo Finance)  
- ETFs  
- AÃ§Ãµes  
- Ouro  
- Renda Fixa  
- Criptomoedas  

### âœ” CÃ¡lculo das features financeiras  
- Retornos diÃ¡rios (`ret`)  
- Volatilidade anualizada (`vol`)  
- Drawdown (`dd`)  
- Clusters setoriais  
- ClassificaÃ§Ã£o do regime de mercado  
  - bull, bear, alta_vol, baixa_vol, neutro  

### âœ” Split do dataset em treino / teste  
- 80% treino  
- 20% teste  

### âœ” ConstruÃ§Ã£o do vetor de estado  
FunÃ§Ã£o: **`discretizar_estado_financeiro()`**  
ContÃ©m:  
- Pesos  
- Retorno mÃ©dio  
- Volatilidade mÃ©dia  
- Drawdown  
- Cluster do ativo  

### âœ” FunÃ§Ã£o de recompensa  
FunÃ§Ã£o: **`calcular_recompensa_portfolio()`**  
Combina:  
- log-retorno  
- penalidade de drawdown  
- penalidade de turnover  

### âœ” FunÃ§Ã£o de aÃ§Ã£o  
FunÃ§Ã£o: **`aplicar_acao_portfolio()`**  
Transforma uma aÃ§Ã£o discreta em um novo vetor de pesos normalizado com limites.

---

# ğŸ•¹ `rl_env.py` â€” O Simulador (Ambiente + Modelos Actor/Critic)

Aqui fica o ambiente estilo **OpenAI Gym**.

## âœ” `PortfolioEnv`
Implementa:

- `reset()`  
- `step()`  
- cÃ¡lculo do valor da carteira  
- cÃ¡lculo do drawdown **real**  
- normalizaÃ§Ã£o dos pesos  
- construÃ§Ã£o do estado  

## âœ” Redes neurais

- `PolicyMLP` â€” **Ator**  
- `ValueMLP` â€” **CrÃ­tico**  

Ambas usando MLP com inicializaÃ§Ã£o Xavier e ReLU.

---

# ğŸ§ª `train_a2c.py` â€” Semana 1 (A2C) â€” Debug / Prova de Conceito

Este script serve para garantir que **todo o pipeline funciona**:

- testagem do ambiente  
- validaÃ§Ã£o dos estados  
- validaÃ§Ã£o da polÃ­tica  
- debug de NaNs  
- execuÃ§Ã£o passo a passo  
- atualizaÃ§Ã£o Actorâ€“Critic padrÃ£o  

O objetivo desta semana Ã© **validar toda a pipeline** antes de evoluir para PPO.

---

# ğŸ¯ `app.py` â€” Sandbox / Testes manuais

Serve como ambiente para:

- testes rÃ¡pidos  
- conferÃªncia de shapes  
- testes de recompensas  
- verificaÃ§Ã£o de normalizaÃ§Ã£o  
- execuÃ§Ã£o da polÃ­tica sem treino  

Ã‰ o **laboratÃ³rio** do projeto.

---

# ğŸš€ `train_ppo.py` â€” PPO (Semana 2) + PPO+GEN (Semana 3)

Este Ã© o arquivo mais importante â€” o modelo final.

## ğŸ”µ Semana 2 â€” PPO puro (sem GAE)
- clipping  
- entropia  
- minibatches  
- mÃºltiplas Ã©pocas por rollout  

Primeira forma estÃ¡vel e efetiva de treinamento.

---

## ğŸŸ£ Semana 3 â€” PPO + GEN (GAE)

Modelo de produÃ§Ã£o:

- GAE(lambda=0.95)  
- vantagem suave e estÃ¡vel  
- treino fixo em dataset de treino  
- avaliaÃ§Ã£o exclusiva no dataset de teste  
- mÃ©tricas:  
  - reward total  
  - log-ret puro  

---

# ğŸ§  DiferenÃ§as entre os modelos

## 1ï¸âƒ£ A2C â€” Simples e frÃ¡gil
âœ” FÃ¡cil para debug  
âœ” RÃ¡pido  
âœ˜ Alta variÃ¢ncia  
âœ˜ InstÃ¡vel  

---

## 2ï¸âƒ£ PPO â€” PadrÃ£o moderno estÃ¡vel
âœ” Clipping evita explosÃµes  
âœ” Treina com minibatches  
âœ” Melhora sobre A2C  
âœ˜ Ainda sofre sem GAE  

---

## 3ï¸âƒ£ PPO + GEN (GAE) â€” Modelo final
âœ” Suaviza vantagens  
âœ” Melhor estabilidade  
âœ” Melhor generalizaÃ§Ã£o  
âœ” Melhor convergÃªncia  

---

# ğŸš€ ExecuÃ§Ã£o

## Semana 1 â€” A2C
```sh
python train_a2c.py
```

## Semana 2 â€” PPO
```sh
python train_ppo.py
```

## Semana 3 â€” PPO + GEN
Mesmo arquivo:
```sh
python train_ppo.py
```

---

# ğŸ“š Funcionalidades

- Simulador realista  
- Penalidade de drawdown  
- Custo de transaÃ§Ã£o  
- AÃ§Ãµes discretas  
- GeneralizaÃ§Ã£o  
- GAE  
- PPO estÃ¡vel  
- Seed fixa (reprodutibilidade)  

---

# ğŸ¤ ContribuiÃ§Ã£o

Melhorias sÃ£o bem-vindas: novas recompensas, novos ambientes, novas redes neurais etc.

---

