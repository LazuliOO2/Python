{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a920a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from teste import (\n",
    "    TICKERS, NUM_ATIVOS, ret, r_media, v_media, dd,\n",
    "    regimes, regime_ids, cluster_ids,\n",
    "    discretizar_estado_financeiro,\n",
    "    calcular_recompensa_portfolio,\n",
    "    aplicar_acao_portfolio\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class PolicyMLP(nn.Module):\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return torch.softmax(logits, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d19dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: torch.Size([1, 37])\n",
      "action_probs shape: torch.Size([1, 34])\n",
      "sum probs: 1.0\n"
     ]
    }
   ],
   "source": [
    "AÇOES_POR_ATIVO = 2\n",
    "state_dim = NUM_ATIVOS + 3 + NUM_ATIVOS\n",
    "num_actions = NUM_ATIVOS * AÇOES_POR_ATIVO\n",
    "\n",
    "taxa_aprendizado = 1e-4\n",
    "policy_net_final = PolicyMLP(state_dim, num_actions)\n",
    "optimizer_final = optim.Adam(policy_net_final.parameters(), lr=taxa_aprendizado)\n",
    "\n",
    "\n",
    "# teste shapes\n",
    "pesos = np.ones(NUM_ATIVOS)/NUM_ATIVOS\n",
    "t = 0\n",
    "state = discretizar_estado_financeiro(\n",
    "    pesos, regime_ids[t], cluster_ids, r_media.iloc[t], v_media.iloc[t],dd.iloc[t]\n",
    ")\n",
    "print(\"state shape:\", state.shape)\n",
    "probs = policy_net_final(state)\n",
    "print(\"action_probs shape:\", probs.shape)\n",
    "print(\"sum probs:\", probs.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c4ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0 | reward -0.00004 | regime neutro\n",
      "Ep 200 | reward -0.00888 | regime neutro\n",
      "Ep 400 | reward -0.01709 | regime alta_vol\n",
      "Ep 600 | reward 0.00637 | regime bull\n",
      "Ep 800 | reward 0.00219 | regime bull\n",
      "Ep 1000 | reward 0.00022 | regime bull\n",
      "Ep 1200 | reward -0.00145 | regime alta_vol\n",
      "Ep 1400 | reward 0.00597 | regime alta_vol\n",
      "Ep 1600 | reward -0.00643 | regime alta_vol\n",
      "Ep 1800 | reward 0.00216 | regime neutro\n",
      "Ep 2000 | reward 0.00373 | regime alta_vol\n",
      "Ep 2200 | reward 0.01910 | regime bull\n",
      "Ep 2400 | reward 0.00657 | regime alta_vol\n",
      "Ep 2600 | reward -0.00671 | regime alta_vol\n",
      "Ep 2800 | reward -0.01313 | regime bear\n",
      "Ep 3000 | reward -0.02348 | regime bear\n",
      "Ep 3200 | reward 0.00250 | regime bear\n",
      "Ep 3400 | reward 0.00162 | regime alta_vol\n",
      "Ep 3600 | reward 0.00824 | regime bull\n",
      "Ep 3800 | reward -0.00233 | regime alta_vol\n"
     ]
    }
   ],
   "source": [
    "episodios = len(ret) - 1\n",
    "pesos = np.ones(NUM_ATIVOS) / NUM_ATIVOS\n",
    "\n",
    "for ep in range(episodios):\n",
    "    t = ep\n",
    "    regime_id = regime_ids[t]\n",
    "    retornos_dia = ret.iloc[t].values\n",
    "    drawdown_dia = dd.iloc[t]\n",
    "    r_dia_media = r_media.iloc[t]\n",
    "    v_dia_media = v_media.iloc[t]\n",
    "\n",
    "    state = discretizar_estado_financeiro(\n",
    "        pesos, regime_id, cluster_ids, r_dia_media, v_dia_media, drawdown_dia \n",
    "    )\n",
    "\n",
    "    action_probs = policy_net_final(state)\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample().item()\n",
    "    log_prob = m.log_prob(torch.tensor(action))\n",
    "\n",
    "    novos_pesos = aplicar_acao_portfolio(pesos, action)\n",
    "\n",
    "    recompensa = calcular_recompensa_portfolio(\n",
    "        pesos_antigos=pesos,\n",
    "        pesos_novos=novos_pesos,\n",
    "        retornos_dia=retornos_dia,\n",
    "        drawdown_dia=drawdown_dia,\n",
    "        lambda_dd=0.2,\n",
    "        lambda_tc=0.001\n",
    "    )\n",
    "\n",
    "    loss = -log_prob * recompensa\n",
    "    optimizer_final.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_final.step()\n",
    "\n",
    "    pesos = novos_pesos\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(f\"Ep {ep} | reward {recompensa:.5f} | regime {regimes[t]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
