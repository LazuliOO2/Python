{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a920a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo encontrado! Carregando dados locais...\n",
      "            BOVA11.SA  SMAL11.SA  IVVB11.SA  PETR4.SA   VALE3.SA  ITUB4.SA  \\\n",
      "Date                                                                         \n",
      "2015-01-01        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-02  47.259998  52.020000  55.799999  2.572509  10.505502  9.372999   \n",
      "2015-01-03        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-04        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-05  46.320000  50.549999  55.750000  2.352637  10.347521  9.420101   \n",
      "\n",
      "                   SPY        QQQ         IWM        EEM         GLD  \\\n",
      "Date                                                                   \n",
      "2015-01-01         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-02  171.093704  94.906532  103.315140  30.789457  114.080002   \n",
      "2015-01-03         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-04         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-05  168.003784  93.514397  101.933861  30.241489  115.800003   \n",
      "\n",
      "                  TLT     BTC-USD  ETH-USD  BNB-USD  SOL-USD  ADA-USD  \n",
      "Date                                                                   \n",
      "2015-01-01        NaN  314.248993      NaN      NaN      NaN      NaN  \n",
      "2015-01-02  95.468956  315.032013      NaN      NaN      NaN      NaN  \n",
      "2015-01-03        NaN  281.082001      NaN      NaN      NaN      NaN  \n",
      "2015-01-04        NaN  264.195007      NaN      NaN      NaN      NaN  \n",
      "2015-01-05  96.968613  274.473999      NaN      NaN      NaN      NaN  \n",
      "(3977, 17)\n",
      "NUM_ATIVOS = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\portfolioDeAtivos\\teste.py:154: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  retornos = precos.pct_change().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.008030805295683797\n",
      "Soma: 1.0000000000000002\n",
      "Min: 0.05767012687427913\n",
      "Max: 0.07727797001153404\n",
      "[0.07727797 0.05767013 0.05767013 0.05767013 0.05767013]\n",
      "Ação 0 | soma=1.000000 | min=0.0577 | max=0.0773\n",
      "Ação 1 | soma=1.000000 | min=0.0565 | max=0.0761\n",
      "Ação 2 | soma=1.000000 | min=0.0554 | max=0.0750\n",
      "Ação 3 | soma=1.000000 | min=0.0543 | max=0.0740\n",
      "Ação 4 | soma=1.000000 | min=0.0533 | max=0.0729\n",
      "Ação 5 | soma=1.000000 | min=0.0522 | max=0.0718\n",
      "Ação 6 | soma=1.000000 | min=0.0512 | max=0.0708\n",
      "Ação 7 | soma=1.000000 | min=0.0502 | max=0.0698\n",
      "Ação 8 | soma=1.000000 | min=0.0492 | max=0.0688\n",
      "Ação 9 | soma=1.000000 | min=0.0483 | max=0.0679\n"
     ]
    }
   ],
   "source": [
    "from teste import (\n",
    "    TICKERS, NUM_ATIVOS, ret, r_media, v_media, dd,\n",
    "    regimes, regime_ids, cluster_ids,\n",
    "    discretizar_estado_financeiro,\n",
    "    calcular_recompensa_portfolio,\n",
    "    aplicar_acao_portfolio\n",
    ")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class PolicyMLP(nn.Module):\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "        return torch.softmax(logits, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d19dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: torch.Size([1, 1, 37])\n",
      "action_probs shape: torch.Size([1, 1, 34])\n",
      "sum probs: 34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olazu\\AppData\\Local\\Temp\\ipykernel_22384\\3151837496.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "AÇOES_POR_ATIVO = 2\n",
    "state_dim = NUM_ATIVOS + 3 + NUM_ATIVOS\n",
    "num_actions = NUM_ATIVOS * AÇOES_POR_ATIVO\n",
    "\n",
    "taxa_aprendizado = 1e-4\n",
    "policy_net_final = PolicyMLP(state_dim, num_actions)\n",
    "optimizer_final = optim.Adam(policy_net_final.parameters(), lr=taxa_aprendizado)\n",
    "\n",
    "\n",
    "# teste shapes\n",
    "pesos = np.ones(NUM_ATIVOS)/NUM_ATIVOS\n",
    "t = 1\n",
    "state = discretizar_estado_financeiro(\n",
    "    pesos, regime_ids[t], cluster_ids, r_media.iloc[t - 1], v_media.iloc[t - 1],dd.iloc[t - 1]\n",
    ")\n",
    "state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "_ = policy_net_final(state)\n",
    "print(\"state shape:\", state.shape)\n",
    "probs = policy_net_final(state)\n",
    "print(\"action_probs shape:\", probs.shape)\n",
    "print(\"sum probs:\", probs.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53c4ae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olazu\\AppData\\Local\\Temp\\ipykernel_22384\\4286855563.py:21: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 200 | reward -0.00682 | regime neutro\n",
      "Ep 400 | reward -0.01431 | regime alta_vol\n",
      "Ep 600 | reward 0.00684 | regime bull\n",
      "Ep 800 | reward 0.00324 | regime bull\n",
      "Ep 1000 | reward -0.00073 | regime bull\n",
      "Ep 1200 | reward -0.00322 | regime alta_vol\n",
      "Ep 1400 | reward 0.00221 | regime alta_vol\n",
      "Ep 1600 | reward 0.00119 | regime alta_vol\n",
      "Ep 1800 | reward 0.00396 | regime neutro\n",
      "Ep 2000 | reward -0.00010 | regime alta_vol\n",
      "Ep 2200 | reward 0.02207 | regime bull\n",
      "Ep 2400 | reward 0.01214 | regime alta_vol\n",
      "Ep 2600 | reward -0.00706 | regime alta_vol\n",
      "Ep 2800 | reward -0.00769 | regime bear\n",
      "Ep 3000 | reward -0.02064 | regime bear\n",
      "Ep 3200 | reward -0.00054 | regime bear\n",
      "Ep 3400 | reward -0.00224 | regime alta_vol\n",
      "Ep 3600 | reward 0.01544 | regime bull\n",
      "Ep 3800 | reward -0.00354 | regime alta_vol\n"
     ]
    }
   ],
   "source": [
    "episodios = len(ret) - 1\n",
    "pesos = np.ones(NUM_ATIVOS) / NUM_ATIVOS\n",
    "\n",
    "for ep in range(1,episodios):\n",
    "    t = ep\n",
    "    t_obs = t - 1\n",
    "\n",
    "    regime_id = regime_ids[t_obs]\n",
    "    \n",
    "    dd_obs = dd.iloc[t_obs]  # oque robo ver\n",
    "    r_dia_media = r_media.iloc[t_obs]\n",
    "    v_dia_media = v_media.iloc[t_obs]\n",
    "\n",
    "    # Retorno do dia \n",
    "    drawdown_dia = dd.iloc[t]\n",
    "    retornos_dia = ret.iloc[t].values\n",
    "    \n",
    "    state = discretizar_estado_financeiro(pesos, regime_id, cluster_ids, r_dia_media, v_dia_media, dd_obs)\n",
    "\n",
    "\n",
    "    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    action_probs = policy_net_final(state)\n",
    "    m = Categorical(action_probs)\n",
    "    action = m.sample().item()\n",
    "    log_prob = m.log_prob(torch.tensor(action))\n",
    "\n",
    "    novos_pesos = aplicar_acao_portfolio(pesos, action)\n",
    "\n",
    "    recompensa = calcular_recompensa_portfolio(\n",
    "        pesos_antigos=pesos,\n",
    "        pesos_novos=novos_pesos,\n",
    "        retornos_dia=retornos_dia,\n",
    "        drawdown_dia=drawdown_dia,\n",
    "        lambda_dd=0.2,\n",
    "        lambda_tc=0.001\n",
    "    )\n",
    "\n",
    "    loss = -log_prob * recompensa\n",
    "    optimizer_final.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer_final.step()\n",
    "\n",
    "    pesos = novos_pesos\n",
    "\n",
    "    if ep % 200 == 0:\n",
    "        print(f\"Ep {ep} | reward {recompensa:.5f} | regime {regimes[t]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
