{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b770bd46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo encontrado! Carregando dados locais...\n",
      "            BOVA11.SA  SMAL11.SA  IVVB11.SA  PETR4.SA   VALE3.SA  ITUB4.SA  \\\n",
      "Date                                                                         \n",
      "2015-01-01        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-02  47.259998  52.020000  55.799999  2.572509  10.505502  9.372999   \n",
      "2015-01-03        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-04        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-05  46.320000  50.549999  55.750000  2.352637  10.347521  9.420101   \n",
      "\n",
      "                   SPY        QQQ         IWM        EEM         GLD  \\\n",
      "Date                                                                   \n",
      "2015-01-01         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-02  171.093704  94.906532  103.315140  30.789457  114.080002   \n",
      "2015-01-03         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-04         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-05  168.003784  93.514397  101.933861  30.241489  115.800003   \n",
      "\n",
      "                  TLT     BTC-USD  ETH-USD  BNB-USD  SOL-USD  ADA-USD  \n",
      "Date                                                                   \n",
      "2015-01-01        NaN  314.248993      NaN      NaN      NaN      NaN  \n",
      "2015-01-02  95.468956  315.032013      NaN      NaN      NaN      NaN  \n",
      "2015-01-03        NaN  281.082001      NaN      NaN      NaN      NaN  \n",
      "2015-01-04        NaN  264.195007      NaN      NaN      NaN      NaN  \n",
      "2015-01-05  96.968613  274.473999      NaN      NaN      NaN      NaN  \n",
      "(3977, 17)\n",
      "NUM_ATIVOS = 17\n",
      "-0.008030805295683797\n",
      "Soma: 1.0000000000000002\n",
      "Min: 0.05767012687427913\n",
      "Max: 0.07727797001153404\n",
      "[0.07727797 0.05767013 0.05767013 0.05767013 0.05767013]\n",
      "Ação 0 | soma=1.000000 | min=0.0577 | max=0.0773\n",
      "Ação 1 | soma=1.000000 | min=0.0565 | max=0.0761\n",
      "Ação 2 | soma=1.000000 | min=0.0554 | max=0.0750\n",
      "Ação 3 | soma=1.000000 | min=0.0543 | max=0.0740\n",
      "Ação 4 | soma=1.000000 | min=0.0533 | max=0.0729\n",
      "Ação 5 | soma=1.000000 | min=0.0522 | max=0.0718\n",
      "Ação 6 | soma=1.000000 | min=0.0512 | max=0.0708\n",
      "Ação 7 | soma=1.000000 | min=0.0502 | max=0.0698\n",
      "Ação 8 | soma=1.000000 | min=0.0492 | max=0.0688\n",
      "Ação 9 | soma=1.000000 | min=0.0483 | max=0.0679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\olazu\\OneDrive\\Documentos\\Python\\portfolioDeAtivos\\portfolioDeAtivos\\teste.py:154: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  retornos = precos.pct_change().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state shape: torch.Size([1, 37])\n",
      "action_probs shape: torch.Size([1, 34])\n",
      "sum probs: 1.0000001192092896\n",
      "Ep 0 | reward -0.00004 | regime neutro\n",
      "Ep 200 | reward -0.00732 | regime neutro\n",
      "Ep 400 | reward -0.02160 | regime alta_vol\n",
      "Ep 600 | reward 0.00132 | regime bull\n",
      "Ep 800 | reward 0.00834 | regime bull\n",
      "Ep 1000 | reward 0.00192 | regime bull\n",
      "Ep 1200 | reward 0.00355 | regime alta_vol\n",
      "Ep 1400 | reward 0.00704 | regime alta_vol\n",
      "Ep 1600 | reward -0.00582 | regime alta_vol\n",
      "Ep 1800 | reward 0.00522 | regime neutro\n",
      "Ep 2000 | reward 0.00164 | regime alta_vol\n",
      "Ep 2200 | reward 0.01999 | regime bull\n",
      "Ep 2400 | reward 0.00675 | regime alta_vol\n",
      "Ep 2600 | reward -0.01032 | regime alta_vol\n",
      "Ep 2800 | reward -0.01639 | regime bear\n",
      "Ep 3000 | reward -0.02224 | regime bear\n",
      "Ep 3200 | reward 0.00079 | regime bear\n",
      "Ep 3400 | reward 0.00215 | regime alta_vol\n",
      "Ep 3600 | reward 0.01224 | regime bull\n",
      "Ep 3800 | reward -0.00771 | regime alta_vol\n",
      "Episódio 0 -> reward total = -3.72957\n",
      "Episódio 1 -> reward total = -3.03851\n",
      "Episódio 2 -> reward total = -2.39913\n",
      "Episódio 3 -> reward total = -3.07908\n",
      "Episódio 4 -> reward total = -1.47909\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "\n",
    "# Importações de módulos locais (presume-se que existam no diretório do projeto)\n",
    "# rl_env: Contém a definição do ambiente de simulação (PortfolioEnv) e as redes neurais (PolicyMLP, ValueMLP)\n",
    "# app: Contém dados financeiros pré-processados e funções auxiliares\n",
    "from rl_env import PortfolioEnv, PolicyMLP, ValueMLP\n",
    "from app import (ret, r_media, v_media, dd, regime_ids, cluster_ids, NUM_ATIVOS)\n",
    "from app import aplicar_acao_portfolio\n",
    "\n",
    "# ==============================================================================\n",
    "# CONFIGURAÇÃO DE HIPERPARÂMETROS E DIMENSÕES\n",
    "# ==============================================================================\n",
    "\n",
    "# Define quantas ações são possíveis por ativo (ex: 2 poderia ser 'comprar' ou 'vender',\n",
    "# ou aumentar/diminuir exposição).\n",
    "AÇÕES_POR_ATIVOS = 2\n",
    "\n",
    "# O espaço de ação total é o número de ativos multiplicado pelas ações possíveis por ativo.\n",
    "# Isso sugere um output 'flat' onde o modelo escolhe um índice que representa (Ativo X, Ação Y).\n",
    "num_actions = NUM_ATIVOS * AÇÕES_POR_ATIVOS\n",
    "\n",
    "# Definição da dimensão do estado (input da rede neural).\n",
    "# A soma abaixo (NUM_ATIVOS + 3 + NUM_ATIVOS) indica que o estado é composto por:\n",
    "# 1. Alocação atual (pesos) dos ativos (NUM_ATIVOS)\n",
    "# 2. Informações de mercado/contexto (3 variáveis, ex: regime, volatilidade, etc)\n",
    "# 3. Alguma outra métrica por ativo ou retornos passados (NUM_ATIVOS)\n",
    "state_dim = NUM_ATIVOS + 3 + NUM_ATIVOS\n",
    "\n",
    "# ==============================================================================\n",
    "# INICIALIZAÇÃO DOS MODELOS E OTIMIZADORES\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "# Inicializa a rede da Política (Actor): Recebe o estado e retorna probabilidades de ação.\n",
    "policy = PolicyMLP(state_dim, num_actions)\n",
    "\n",
    "# Inicializa a rede de Valor (Critic): Recebe o estado e estima o valor (V) daquele estado.\n",
    "value = ValueMLP(state_dim)\n",
    "\n",
    "def init_weights(m):\n",
    "# Checa se o módulo é uma camada totalmente conectada\n",
    "    if isinstance(m, nn.Linear):\n",
    "# Define os pesos dessa camada com a inicialização de Xavier (Glorot\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "# Atribui zero aos vieses\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "policy.apply(init_weights)\n",
    "value.apply(init_weights)\n",
    "\n",
    "\n",
    "# Configura os otimizadores (Adam) para ajustar os pesos das redes.\n",
    "# lr=1e-4 é a taxa de aprendizado (learning rate).\n",
    "optimizerP = optim.Adam(policy.parameters(), lr=1e-4)\n",
    "optimizerV = optim.Adam(value.parameters(), lr=1e-4)\n",
    "\n",
    "# ==============================================================================\n",
    "# INICIALIZAÇÃO DO AMBIENTE\n",
    "# ==============================================================================\n",
    "\n",
    "# Cria a instância do ambiente de portfólio passando os dados financeiros necessários.\n",
    "env = PortfolioEnv(ret, r_media, v_media, dd, regime_ids, cluster_ids)\n",
    "\n",
    "# ==============================================================================\n",
    "# LOOP DE TREINAMENTO (EPISÓDIOS)\n",
    "# ==============================================================================\n",
    "\n",
    "episodios = 5   # Número de episódios para rodar (valor baixo apenas para debug/teste)\n",
    "\n",
    "for ep in range(episodios):\n",
    "    # Reseta o ambiente para o estado inicial no começo de cada episódio\n",
    "    state = env.reset()\n",
    "    \n",
    "    # Converte o estado (numpy array) para Tensor do PyTorch e adiciona dimensão de batch (unsqueeze)\n",
    "    state = torch.FloatTensor(state).unsqueeze(0)\n",
    "\n",
    "    done = False        # Flag para controlar o fim do episódio\n",
    "    total_reward = 0    # Acumulador de recompensa do episódio\n",
    "\n",
    "    # Loop principal de interação passo-a-passo (timesteps) dentro do episódio\n",
    "    while not done:\n",
    "\n",
    "        # --- 1. Escolha da Ação (Actor) ---\n",
    "        \n",
    "        \n",
    "        logits = policy(state)\n",
    "\n",
    "        \n",
    "        if torch.isnan(logits).any() or not torch.isfinite(logits).all():\n",
    "            print(\"DEU RUIM: logits inválidos (NaN/Inf)\")\n",
    "            print(\"state:\", state)\n",
    "            print(\"logits:\", logits)\n",
    "            break\n",
    "        \n",
    "        \n",
    "        m = Categorical(logits=logits)\n",
    "        \n",
    "        # Amostra uma ação baseada na distribuição (exploração estocástica)\n",
    "        action = m.sample()\n",
    "        \n",
    "        # Calcula o log da probabilidade da ação escolhida (necessário para calcular a loss depois)\n",
    "        log_prob = m.log_prob(action)\n",
    "\n",
    "        # --- 2. Execução no Ambiente ---\n",
    "\n",
    "        # Função auxiliar que traduz o índice da ação (int) para o novo vetor de pesos do portfólio\n",
    "        novos_pesos = aplicar_acao_portfolio(env.pesos, action.item())\n",
    "\n",
    "        # O ambiente dá um passo com os novos pesos e retorna:\n",
    "        # next_state: O novo estado do mercado/portfólio\n",
    "        # reward: A recompensa imediata (ex: retorno financeiro, Sharpe ratio, etc)\n",
    "        # done: Se o episódio acabou (ex: fim dos dados históricos ou falência)\n",
    "        next_state, reward, done = env.step(novos_pesos)\n",
    "\n",
    "        # Prepara o próximo estado para processamento pelo PyTorch\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "\n",
    "        # --- 3. Cálculo do Valor e Advantage (Critic) ---\n",
    "\n",
    "        # Estima o valor do estado ATUAL V(s)\n",
    "        V = value(state)\n",
    "        \n",
    "        # Estima o valor do PRÓXIMO estado V(s'). .detach() é usado porque não queremos\n",
    "        # propagar gradientes através desta estimativa alvo (target).\n",
    "        V_next = value(next_state_tensor).detach()\n",
    "\n",
    "        # Calcula o Advantage (Vantagem) usando TD Error (Temporal Difference de 1 passo):\n",
    "        # A = Reward + (gamma * V_next) - V_current\n",
    "        # gamma aqui é 0.99 (fator de desconto). \n",
    "        # (1 - int(done)) garante que se o episódio acabou, o valor futuro é 0.\n",
    "        advantage = reward + (0.99 * V_next * (1 - int(done))) - V\n",
    "\n",
    "        # --- 4. Cálculo das Perdas (Losses) e Backpropagation ---\n",
    "\n",
    "        # Policy Loss (Actor Loss): Maximizar log_prob * advantage.\n",
    "        # Como o otimizador minimiza, usamos o sinal negativo (-).\n",
    "        # .detach() no advantage é crucial para não atualizar o Critic via loss do Actor.\n",
    "        policy_loss = -(log_prob * advantage.detach())\n",
    "\n",
    "        # Value Loss (Critic Loss): Erro quadrático médio entre a estimativa de valor e o alvo (TD Target).\n",
    "        # Basicamente: queremos que V(s) preveja corretamente Reward + V(s').\n",
    "        value_loss = advantage.pow(2)\n",
    "\n",
    "        # Atualização da rede da Política (Actor)\n",
    "        optimizerP.zero_grad()      # Zera gradientes anteriores\n",
    "        policy_loss.backward()      # Calcula gradientes\n",
    "        optimizerP.step()           # Atualiza pesos\n",
    "\n",
    "        # Atualização da rede de Valor (Critic)\n",
    "        optimizerV.zero_grad()\n",
    "        value_loss.backward()\n",
    "        optimizerV.step()\n",
    "\n",
    "        # --- 5. Atualização de Estado ---\n",
    "        \n",
    "        # O próximo estado vira o estado atual para a próxima iteração\n",
    "        state = next_state_tensor\n",
    "        total_reward += reward\n",
    "\n",
    "    # Fim do episódio\n",
    "    print(f\"Episódio {ep} -> reward total = {total_reward:.5f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
