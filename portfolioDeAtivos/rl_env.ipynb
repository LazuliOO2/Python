{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f1e7b3",
   "metadata": {},
   "source": [
    "A classe PortfólioEnv é onde o robô vai operar e tomar suas decisões. Ela recebe como parâmetros para aprender:\n",
    "self, que é obrigatório em Python,\n",
    "rentabilidade,\n",
    "média da rentabilidade,\n",
    "média de volatilidade,\n",
    "drawdown,\n",
    "regime (que indica como o mercado está, por exemplo: otimista, pessimista, alta volatilidade etc.),\n",
    "e clusters, que representam agrupamentos por setor.\n",
    "\n",
    "Rentabilidade (Retorno): É o \"quanto variou\". Se a ação valia R$ 100 e foi para R$ 110, o retorno é 0.10 (ou 10%). Se caiu para R$ 90, é -0.10.\n",
    "\n",
    "Volatilidade: É o \"quanto chacoalha\".\n",
    "\n",
    "Uma ação que faz +0.1%, -0.1%, +0.2% tem baixa volatilidade (calma).\n",
    "\n",
    "Uma ação que faz +5%, -10%, +15% tem alta volatilidade (nervosa/arriscada)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e1ee67",
   "metadata": {},
   "source": [
    "3. Pesos: Confiança ou Aposta?\n",
    "Você perguntou: \"peso é confiança do modelo certo? 25% significa que ele ta confiante nessa quantidade para aposta naquela ação\"\n",
    "\n",
    "Resposta: Sim e Não.\n",
    "\n",
    "Tecnicamente: O peso é a Alocação de Capital. É a \"Aposta\" física. Significa: \"Do dinheiro que eu tenho no bolso, vou colocar 25% aqui\".\n",
    "\n",
    "Estrategicamente: Sim, reflete a Confiança.\n",
    "\n",
    "Se o modelo (Rede Neural) tem certeza absoluta que a ação vai subir, ele vai tentar jogar o peso para 1.0 (100% do dinheiro).\n",
    "\n",
    "Se ele está em dúvida ou acha que vai cair, ele diminui o peso para 0.0 ou algo muito baixo.\n",
    "\n",
    "Então, o peso é a materialização da confiança.\n",
    "\n",
    "Cuidado: O modelo sempre tenta maximizar o lucro. Se você não colocar limites, ele pode ter \"excesso de confiança\" e colocar 100% em uma ação arriscada. Por isso, em sistemas reais, costumamos limitar os pesos (ex: no máximo 30% por ação) para forçar ele a diversificar, mesmo que ele esteja \"muito confiante\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "095d7cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo encontrado! Carregando dados locais...\n",
      "            BOVA11.SA  SMAL11.SA  IVVB11.SA  PETR4.SA   VALE3.SA  ITUB4.SA  \\\n",
      "Date                                                                         \n",
      "2015-01-01        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-02  47.259998  52.020000  55.799999  2.572509  10.505502  9.372999   \n",
      "2015-01-03        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-04        NaN        NaN        NaN       NaN        NaN       NaN   \n",
      "2015-01-05  46.320000  50.549999  55.750000  2.352637  10.347521  9.420101   \n",
      "\n",
      "                   SPY        QQQ         IWM        EEM         GLD  \\\n",
      "Date                                                                   \n",
      "2015-01-01         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-02  171.093704  94.906532  103.315140  30.789457  114.080002   \n",
      "2015-01-03         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-04         NaN        NaN         NaN        NaN         NaN   \n",
      "2015-01-05  168.003784  93.514397  101.933861  30.241489  115.800003   \n",
      "\n",
      "                  TLT     BTC-USD  ETH-USD  BNB-USD  SOL-USD  ADA-USD  \n",
      "Date                                                                   \n",
      "2015-01-01        NaN  314.248993      NaN      NaN      NaN      NaN  \n",
      "2015-01-02  95.468956  315.032013      NaN      NaN      NaN      NaN  \n",
      "2015-01-03        NaN  281.082001      NaN      NaN      NaN      NaN  \n",
      "2015-01-04        NaN  264.195007      NaN      NaN      NaN      NaN  \n",
      "2015-01-05  96.968613  274.473999      NaN      NaN      NaN      NaN  \n",
      "(3977, 17)\n",
      "NUM_ATIVOS = 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\portfolioDeAtivos\\teste.py:154: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  retornos = precos.pct_change().fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.008030805295683797\n",
      "Soma: 1.0000000000000002\n",
      "Min: 0.05767012687427913\n",
      "Max: 0.07727797001153404\n",
      "[0.07727797 0.05767013 0.05767013 0.05767013 0.05767013]\n",
      "Ação 0 | soma=1.000000 | min=0.0577 | max=0.0773\n",
      "Ação 1 | soma=1.000000 | min=0.0565 | max=0.0761\n",
      "Ação 2 | soma=1.000000 | min=0.0554 | max=0.0750\n",
      "Ação 3 | soma=1.000000 | min=0.0543 | max=0.0740\n",
      "Ação 4 | soma=1.000000 | min=0.0533 | max=0.0729\n",
      "Ação 5 | soma=1.000000 | min=0.0522 | max=0.0718\n",
      "Ação 6 | soma=1.000000 | min=0.0512 | max=0.0708\n",
      "Ação 7 | soma=1.000000 | min=0.0502 | max=0.0698\n",
      "Ação 8 | soma=1.000000 | min=0.0492 | max=0.0688\n",
      "Ação 9 | soma=1.000000 | min=0.0483 | max=0.0679\n"
     ]
    }
   ],
   "source": [
    "from teste import (\n",
    "    TICKERS, NUM_ATIVOS, ret, r_media, v_media, dd,\n",
    "    regimes, regime_ids, cluster_ids,\n",
    "    discretizar_estado_financeiro,\n",
    "    calcular_recompensa_portfolio,\n",
    "    aplicar_acao_portfolio\n",
    ")\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#? É o \"mundo\" onde o seu robô (agente) vai viver, tomar decisões e aprender.\n",
    "class PortfolioEnv:\n",
    "    def __init__(self, ret, r_media, v_media, dd, regime_ids, cluster_ids):\n",
    "        self.ret = ret\n",
    "        self.r_media = r_media\n",
    "        self.v_media = v_media\n",
    "        self.dd = dd\n",
    "        self.regime_ids = regime_ids\n",
    "        self.cluster_ids = cluster_ids\n",
    "        \n",
    "        # len igual tamanho total ou seja se temos dados de 3 dias o len conta as linha e retorna e nesse explemo o jogo do robo duraria 3 estado\n",
    "        self.num_steps = len(ret) # # Duração total do jogo (ex: 1000 dias)\n",
    "        \n",
    "        # .shape (forma) devolve as dimensões da tabela no formato (linhas, colunas).Quando usamos o indice 1 pegamos o números de colunas\n",
    "        self.num_assets = ret.shape[1] # # Quantas ações temos (ex: 10 ações)\n",
    "        self.t = 1\n",
    "        \n",
    "        self.valor_carteira = 1.0\n",
    "        self.pico_historico = 1.0\n",
    "        # np.ones(N) cria uma lista de tamanho N preenchida apenas com o número 1.\n",
    "        # np.ones(4): O computador cria isso: [1, 1, 1, 1] / self.num_assets: Ele divide todos os números por 4. Ou seja inicia o jogo com dinheiro dividido igualmente entre as ações\n",
    "        self.pesos = np.ones(self.num_assets) / self.num_assets\n",
    "\n",
    "    #? Reiniciando a Partida\n",
    "    def reset(self):\n",
    "        self.t = 1 # inciia o jogo no dia 1\n",
    "        self.pesos = np.ones(self.num_assets) / self.num_assets # Divide dinheiro igualmente entre as ações\n",
    "\n",
    "        self.valor_carteira = 1.0  # Começa com 100% do capital (normalizado em 1.0)\n",
    "        self.pico_historico = 1.0  # O topo histórico inicial é o próprio valor inicial\n",
    "        \n",
    "        # seria a visão do robô\n",
    "        state = self._get_state()\n",
    "        return state\n",
    "\n",
    "    # O \"Dia a Dia\" (A parte mais importante)\n",
    "    def step(self, novos_pesos):\n",
    "        # 1. Pega os retornos do mercado hoje\n",
    "        retornos_dia = self.ret.iloc[self.t].values\n",
    "\n",
    "        # --- CÁLCULO DA CARTEIRA DO ROBÔ (A Lógica Nova) ---\n",
    "        \n",
    "        # Calcula quanto a carteira rendeu com os novos pesos\n",
    "        # Nota: Estamos assumindo que ele rebalanceou e pegou o retorno do dia (simplificação comum em RL)\n",
    "        r_p = np.dot(novos_pesos, retornos_dia)\n",
    "\n",
    "        # Atualiza o valor total da carteira (Juros compostos)\n",
    "        self.valor_carteira *= (1 + r_p)\n",
    "\n",
    "        # Atualiza o topo histórico (High Water Mark)\n",
    "        if self.valor_carteira > self.pico_historico:\n",
    "            self.pico_historico = self.valor_carteira\n",
    "\n",
    "        # Calcula o Drawdown REAL dele (Quanto caiu desde o topo?)\n",
    "        # Se valor = 0.95 e pico = 1.00 -> 0.95/1.0 - 1 = -0.05 (-5%)\n",
    "        meu_drawdown = (self.valor_carteira / self.pico_historico) - 1\n",
    "        \n",
    "        # ----------------------------------------------------\n",
    "\n",
    "        pesos_antigos = self.pesos.copy()\n",
    "\n",
    "        # Calcula recompensa usando o SEU drawdown e o lambda ajustado\n",
    "        reward = calcular_recompensa_portfolio(\n",
    "            pesos_antigos=pesos_antigos,\n",
    "            pesos_novos=novos_pesos,\n",
    "            retornos_dia=retornos_dia,\n",
    "            drawdown_dia=meu_drawdown,  # <--- AGORA USA O DD DO ROBÔ\n",
    "            lambda_dd=0.02,              # <--- PUNICAO JUSTA (0.2)\n",
    "            lambda_tc=0.001\n",
    "        )\n",
    "\n",
    "        # Atualiza estado\n",
    "        self.pesos = novos_pesos\n",
    "        self.t += 1\n",
    "        done = (self.t >= self.num_steps - 1)\n",
    "\n",
    "        state = self._get_state()\n",
    "        return state, reward, done\n",
    "\n",
    "    #? A \"Visão\" do Robô\n",
    "    def _get_state(self):\n",
    "\n",
    "        # o que o robô enxerga é o passado (t-1)\n",
    "        t_obs = self.t - 1\n",
    "        # np.concatenate unir vários array e claro a gente transforam todos eles no tipo float32\n",
    "        state = np.concatenate([\n",
    "            self.pesos, # Como está minha carteira agora?\n",
    "            [self.r_media.iloc[t_obs]], # Qual a média de retorno recente?\n",
    "            [self.v_media.iloc[t_obs]], # Qual a volatilidade recente?\n",
    "            [self.dd.iloc[t_obs]], \n",
    "            np.array(self.cluster_ids) # A qual setor cada ação pertence?\n",
    "        ]).astype(np.float32)\n",
    "  \n",
    "        state = np.nan_to_num(state, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e350c",
   "metadata": {},
   "source": [
    "PolicyMLP = O Ator (O Jogador)\n",
    "\n",
    "O que faz:\n",
    "    Ele olha para o estado do jogo e decide qual ação tomar.\n",
    "\n",
    "Saída:\n",
    "    Probabilidades (Ex: 80% pular, 20% correr).\n",
    "\n",
    "Objetivo:\n",
    "    Aprender a escolher as ações que dão mais recompensa.\n",
    "\n",
    "No código:\n",
    "    É por isso que termina com softmax. Ele precisa escolher uma ação.\n",
    "\n",
    "\n",
    "ValueMLP = O Crítico (O Treinador)\n",
    "\n",
    "O que faz:\n",
    "    Ele não joga. Ele olha para o estado e diz \"quão boa é essa situação\".\n",
    "\n",
    "Saída:\n",
    "    Um único número real (chamado de Valor ou V(s)).\n",
    "\n",
    "Exemplo:\n",
    "    Se o boneco está prestes a cair num buraco:\n",
    "        Valor baixo (ex: -100).\n",
    "\n",
    "    Se está perto de ganhar:\n",
    "        Valor alto (ex: 500).\n",
    "\n",
    "Por que treinamos ele?\n",
    "    Para \"guiar\" o Ator.\n",
    "\n",
    "    Muitas vezes, a recompensa do jogo demora para chegar\n",
    "    (só ganha ponto no final da fase). O Ator ficaria perdido\n",
    "    sem saber se está indo bem.\n",
    "\n",
    "    O ValueMLP aprende a prever essa recompensa futura.\n",
    "    Ele serve como uma bússola imediata para o Ator.\n",
    "\n",
    "\n",
    "Resumido um modelo para jogar e outro modelo para avaliar o desempenho do jogador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "676e76bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#saida = x * W + b\n",
    "# ativação relu = max(0, saida) Ela zera valores negativos e deixa positivos como estão.\n",
    "import torch.nn as nn\n",
    "# Isso cria uma rede neural personalizada herdando de nn.Module\n",
    "class PolicyMLP(nn.Module):\n",
    "# state_dim é o tamanho do vetor de entrada (dimensão do estado). num_actions é quantas ações o agente pode tomar\n",
    "    def __init__(self, state_dim, num_actions):\n",
    "# O super() só registra o módulo na infraestrutura do PyTorch\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "# na camada linear: estado → 128 neurônios.Primeiro parametro é entrada e a segunda é quantidade de neuronios\n",
    "            nn.Linear(state_dim, 128), \n",
    "# ReLU como ativação           \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_actions)\n",
    "        )\n",
    "#A entrada x passa pela rede e vira logits.Logits são valores crus, sem normalização\n",
    "    def forward(self, x):\n",
    "        logits = self.net(x)\n",
    "# softmax converte isso numa distribuição de probabilidade sobre as ações (cada linha soma 1).dim=1 significa: aplicar softmax ao longo das colunas, isto é, entre ações.\n",
    "        return logits\n",
    "    \n",
    "# Qual bom ele está nessse estado ? \n",
    "class ValueMLP(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
